#!/usr/bin/env python3
"""
Enhanced Backup Metrics Collector with systemd log parsing and InfluxDB integration
"""
import os
import sys
import re
import json
import subprocess
import time
import threading
import queue
import hashlib
import pickle
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

# Try to import optional dependencies
try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False
    yaml = None

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    requests = None

class BackupMetricsCollector:
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        
        # Set default config file paths with search order
        self.config_file = self._find_config_file("backup.yaml")
        self.influxdb_config_file = self._find_config_file("influxdb-config.yaml")
        
        # Change state tracking to use timestamps instead of hashes
        self.state_file = self._find_config_file("metrics-timestamps.pkl")
        self.sent_timestamps = self._load_sent_timestamps_state()
            
        self.metrics = []
        self.load_config()
        self.load_influxdb_config()
        
        # Thread management
        self.du_thread = None
        self.du_results = {}
        self.du_queue = queue.Queue()
        self.du_completed = threading.Event()

    def _find_config_file(self, filename: str) -> str:
        """Find configuration file in search paths: ./ then /etc/backup/"""
        search_paths = [
            f"./{filename}",
            f"/etc/backup/{filename}"
        ]
        
        for path in search_paths:
            if Path(path).exists():
                return path
        
        # Return the /etc/backup path as default if none found
        return f"/etc/backup/{filename}"
    
    def load_config(self):
        """Load backup configuration to get list of hosts"""
        try:
            with open(self.config_file, 'r') as f:
                self.config = yaml.safe_load(f)
            self.hosts = list(self.config.get('hosts', {}).keys())
            print(f"Loaded backup config from: {self.config_file}")
        except Exception as e:
            print(f"Warning: Could not load config {self.config_file}: {e}")
            self.config = {}
            self.hosts = []
    
    def load_influxdb_config(self):
        """Load InfluxDB configuration"""
        try:
            with open(self.influxdb_config_file, 'r') as f:
                self.influxdb_config = yaml.safe_load(f)
            print(f"Loaded InfluxDB config from: {self.influxdb_config_file}")
            
        except Exception as e:
            print(f"Warning: Could not load InfluxDB config {self.influxdb_config_file}: {e}")
            # Set default values
            self.influxdb_config = {
                'influxdb': {
                    'host': 'localhost',
                    'port': 8086,
                    'database': 'backup_metrics',
                    'username': '',
                    'password': '',
                    'ssl': False,
                    'organization': '',
                    'bucket': '',
                    'token': ''
                }
            }

    def _parse_btrfs_du_output(self, output: str, path: str) -> int:
        """Parse btrfs filesystem du output and return size in bytes"""
        try:
            # btrfs filesystem du output format:
            #      Total   Exclusive  Set shared  Filename
            #   123.45MB    123.45MB       0.00B  /path/to/directory
            lines = output.strip().split('\n')
            for line in lines:
                if path in line:
                    # Extract the first size value (Total)
                    parts = line.split()
                    if len(parts) >= 1:
                        size_str = parts[0]
                        return self._parse_size_string(size_str)
            
            # If path not found, try to get the last line with size data
            for line in reversed(lines):
                if any(unit in line.upper() for unit in ['B', 'KB', 'MB', 'GB', 'TB']):
                    parts = line.split()
                    if len(parts) >= 1:
                        size_str = parts[0]
                        return self._parse_size_string(size_str)
        except Exception as e:
            if self.verbose:
                print(f"Error parsing btrfs du output: {e}")
        return 0

    def _parse_size_string(self, size_str: str) -> int:
        """Parse size string like '123.45MB' and return bytes"""
        try:
            # Remove any commas and normalize
            size_str = size_str.replace(',', '').upper()
            
            # Extract number and unit
            import re
            match = re.match(r'^([\d.]+)([KMGT]?B?)$', size_str)
            if not match:
                return int(float(size_str))  # Assume bytes if no unit
            
            number, unit = match.groups()
            number = float(number)
            
            # Convert to bytes
            multipliers = {
                'B': 1,
                'KB': 1024,
                'MB': 1024**2,
                'GB': 1024**3,
                'TB': 1024**4
            }
            
            multiplier = multipliers.get(unit, 1)
            return int(number * multiplier)
        except Exception as e:
            if self.verbose:
                print(f"Error parsing size string '{size_str}': {e}")
            return 0

    def _parse_du_output(self, output: str) -> int:
        """Parse standard du output and return size in bytes"""
        try:
            # du output format: "size_in_kb    /path"
            lines = output.strip().split('\n')
            if lines:
                # Get the last line (total)
                last_line = lines[-1]
                size_kb = int(last_line.split()[0])
                return size_kb * 1024  # Convert KB to bytes
        except Exception as e:
            if self.verbose:
                print(f"Error parsing du output: {e}")
        return 0

    def _count_files_fast(self, path: str) -> int:
        """Fast file counting using find with limits"""
        try:
            # Use find with a reasonable limit to avoid hanging
            result = subprocess.run(
                ['find', path, '-type', 'f', '-print0'],
                capture_output=True, text=True, timeout=300  # 5 minute timeout
            )
            if result.returncode == 0:
                # Count null-separated entries
                return result.stdout.count('\0')
        except subprocess.TimeoutExpired:
            if self.verbose:
                print(f"File count timeout for {path}")
        except Exception as e:
            if self.verbose:
                print(f"Error counting files in {path}: {e}")
        return 0

    def _add_basic_stats(self, hostname: str, host_dir: Path):
        """Add basic file stats as fallback when du fails"""
        try:
            stat_info = host_dir.stat()
            stats = {
                'size_bytes': 0,  # Unknown size
                'file_count': 0,  # Unknown count
                'last_modified': datetime.fromtimestamp(stat_info.st_mtime),
                'path': str(host_dir),
                'scan_duration': 0,
                'scan_method': 'basic_fallback'
            }
            self.du_queue.put((hostname, stats))
            print(f"⚠ (using basic stats)")
        except Exception as e:
            print(f"⚠ (basic stats failed: {e})")

    def _directory_scanner_worker(self, host_dirs: List[Path], backup_base: str):
        """Background worker thread for scanning directories - uses btrfs filesystem du when available"""
        print(f"Starting background directory scan for {len(host_dirs)} directories...")
        
        # Check if we're on a btrfs filesystem
        btrfs_available = self._is_btrfs_filesystem(backup_base) and self._command_exists("btrfs")
        if btrfs_available:
            print("Using btrfs filesystem du for improved performance")
        else:
            print("Using standard du command")
        
        for i, host_dir in enumerate(host_dirs):
            hostname = host_dir.name
            print(f"Background scan {i+1}/{len(host_dirs)}: {hostname}...", end=" ", flush=True)
            
            try:
                start_time = time.time()
                
                if btrfs_available:
                    du_cmd = ['btrfs', 'filesystem', 'du', '-s', str(host_dir)]
                else:
                    du_cmd = ['du', '-sb', str(host_dir)]
                
                # Reduce timeout for very slow operations
                timeout_duration = 1800 if btrfs_available else 3600  # 30min for btrfs, 1hr for du
                
                result = subprocess.run(
                    du_cmd,
                    capture_output=True, text=True, check=True,
                    timeout=timeout_duration
                )
                
                if btrfs_available:
                    size_bytes = self._parse_btrfs_du_output(result.stdout, str(host_dir))
                else:
                    size_bytes = self._parse_du_output(result.stdout)
                
                scan_duration = time.time() - start_time
                
                # Skip file count for very large directories to improve performance
                if scan_duration > 60:
                    file_count = 0  # Skip file counting for slow directories
                else:
                    file_count = self._count_files_fast(str(host_dir))
                
                last_modified = host_dir.stat().st_mtime
                
                stats = {
                    'size_bytes': size_bytes,
                    'file_count': file_count,
                    'last_modified': datetime.fromtimestamp(last_modified),
                    'path': str(host_dir),
                    'scan_duration': scan_duration,
                    'scan_method': 'btrfs' if btrfs_available else 'du'
                }
                
                self.du_queue.put((hostname, stats))
                print(f"✓ ({size_bytes / 1024**3:.1f}GB in {scan_duration:.1f}s)")
                
                # Send individual host update to InfluxDB immediately
                self._send_host_metrics_to_influxdb(hostname, stats)
                
            except subprocess.CalledProcessError as e:
                print(f"⚠ (error: {e})")
                # If btrfs fails, try fallback to regular du
                if btrfs_available:
                    try:
                        fallback_cmd = ['du', '-sb', str(host_dir)]
                        result = subprocess.run(fallback_cmd, capture_output=True, text=True, check=True, timeout=1800)
                        size_bytes = self._parse_du_output(result.stdout)
                        file_count = 0  # Skip file count for fallback
                        scan_duration = time.time() - start_time
                        last_modified = host_dir.stat().st_mtime
                        
                        stats = {
                            'size_bytes': size_bytes,
                            'file_count': file_count,
                            'last_modified': datetime.fromtimestamp(last_modified),
                            'path': str(host_dir),
                            'scan_duration': scan_duration,
                            'scan_method': 'du_fallback'
                        }
                        self.du_queue.put((hostname, stats))
                        print(f"✓ fallback ({size_bytes / 1024**3:.1f}GB)")
                    except Exception as fallback_error:
                        print(f"⚠ (fallback failed: {fallback_error})")
                        self._add_basic_stats(hostname, host_dir)
                else:
                    self._add_basic_stats(hostname, host_dir)
                
            except subprocess.TimeoutExpired:
                timeout_min = timeout_duration // 60
                print(f"⚠ (timeout after {timeout_min} minutes)")
                # Use basic file stats as fallback for timeout
                self._add_basic_stats(hostname, host_dir)
        
        print("Background directory scan completed!")
        self.du_completed.set()

    def _command_exists(self, command: str) -> bool:
        """Check if a command exists on the system"""
        try:
            subprocess.run(["which", command], check=True, capture_output=True)
            return True
        except subprocess.CalledProcessError:
            return False
    
    def _is_btrfs_filesystem(self, path: str) -> bool:
        """Check if a path is on a btrfs filesystem"""
        try:
            result = subprocess.run(
                ["df", "-T", path], 
                capture_output=True, text=True, check=True
            )
            return "btrfs" in result.stdout
        except subprocess.CalledProcessError:
            return False
    
    def parse_rsync_statistics(self, hours: int = 24) -> Dict:
        """Parse rsync statistics from backup logs"""
        logs = self.parse_systemd_logs(hours=hours)
        rsync_stats = {}
        
        # Track current host/path context
        current_host = None
        current_path = None
        
        for log in logs:
            message = log.get('MESSAGE', '')
            
            # Check for host context
            host_match = re.search(r'=== Starting backup for (\w+)', message)
            if host_match:
                current_host = host_match.group(1)
                if current_host not in rsync_stats:
                    rsync_stats[current_host] = {}
            
            # Check for path context
            path_match = re.search(r'Volume backup .+?: \w+:([^\s\(]+)', message)
            if path_match:
                current_path = path_match.group(1).strip()
                if current_host and current_path and current_path not in rsync_stats.get(current_host, {}):
                    rsync_stats.setdefault(current_host, {})[current_path] = {
                        'bytes_sent': 0,
                        'bytes_received': 0,
                        'transfer_rate': 0,
                        'total_size': 0,
                        'speedup': 1.0  # Default speedup
                    }
            
            # Look for rsync summary statistics
            # Pattern like: "sent 1,234,567 bytes  received 890 bytes  123,456.78 bytes/sec"
            rsync_match = re.search(
                r'sent ([\d,\.]+) bytes\s+received ([\d,\.]+) bytes\s+([\d,\.]+) bytes/sec', 
                message
            )
            if rsync_match and current_host and current_path:
                # Convert European number format (1.234,56) to standard format for parsing
                bytes_sent_str = rsync_match.group(1).replace('.', '').replace(',', '.')
                bytes_received_str = rsync_match.group(2).replace('.', '').replace(',', '.')
                transfer_rate_str = rsync_match.group(3).replace('.', '').replace(',', '.')
                
                try:
                    bytes_sent = int(float(bytes_sent_str))
                    bytes_received = int(float(bytes_received_str))
                    transfer_rate = float(transfer_rate_str)
                    
                    if current_host not in rsync_stats:
                        rsync_stats[current_host] = {}
                    
                    if current_path not in rsync_stats[current_host]:
                        rsync_stats[current_host][current_path] = {}
                    
                    rsync_stats[current_host][current_path].update({
                        'bytes_sent': bytes_sent,
                        'bytes_received': bytes_received,
                        'transfer_rate': transfer_rate,
                        'total_size': bytes_sent + bytes_received,
                    })
                    
                    if self.verbose:
                        print(f"Found rsync stats for {current_host}:{current_path} - sent: {bytes_sent}, received: {bytes_received}, rate: {transfer_rate}")
                        
                except (ValueError, TypeError) as e:
                    if self.verbose:
                        print(f"Error parsing rsync numbers: {e} - {bytes_sent_str}, {bytes_received_str}, {transfer_rate_str}")
            
            # Look for speedup information
            speedup_match = re.search(r'total size is ([\d,\.]+)\s+speedup is ([\d,\.]+)', message)
            if speedup_match and current_host and current_path:
                # Convert European number format (1.234,56) to standard format for parsing
                total_size_str = speedup_match.group(1).replace('.', '').replace(',', '.')
                speedup_str = speedup_match.group(2).replace('.', '').replace(',', '.')
                
                try:
                    total_size = int(float(total_size_str))
                    speedup = float(speedup_str)
                    
                    if current_host not in rsync_stats:
                        rsync_stats[current_host] = {}
                    
                    if current_path not in rsync_stats[current_host]:
                        rsync_stats[current_host][current_path] = {}
                    
                    rsync_stats[current_host][current_path].update({
                        'total_size': total_size,
                        'speedup': speedup
                    })
                    
                    if self.verbose:
                        print(f"Found speedup stats for {current_host}:{current_path} - total size: {total_size}, speedup: {speedup}")
                        
                except (ValueError, TypeError) as e:
                    if self.verbose:
                        print(f"Error parsing speedup numbers: {e} - {total_size_str}, {speedup_str}")
                
        return rsync_stats

    def _load_sent_timestamps_state(self) -> set:
        """Load previously sent timestamp markers from state file"""
        try:
            if Path(self.state_file).exists():
                with open(self.state_file, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            if self.verbose:
                print(f"Could not load timestamp state file {self.state_file}: {e}")
        return set()

    def _save_sent_timestamps_state(self):
        """Save sent timestamp markers to state file"""
        try:
            # Only keep timestamps from the last 30 days to prevent file growth
            cutoff_time = time.time() - (30 * 24 * 3600)
            
            # Filter out old entries
            current_timestamps = {ts for ts in self.sent_timestamps if ts > cutoff_time}
            
            with open(self.state_file, 'wb') as f:
                pickle.dump(current_timestamps, f)
                
            if self.verbose:
                print(f"Saved {len(current_timestamps)} timestamp markers to state file")
                
        except Exception as e:
            if self.verbose:
                print(f"Could not save timestamp state file {self.state_file}: {e}")

    def _get_precise_timestamp(self, log_entry: Dict) -> Optional[float]:
        """Extract precise timestamp from systemd log entry in microseconds"""
        try:
            # Try __REALTIME_TIMESTAMP first (microseconds since epoch)
            realtime = log_entry.get('__REALTIME_TIMESTAMP')
            if realtime:
                return float(realtime) / 1_000_000  # Convert to seconds
            
            # Fallback to _SOURCE_REALTIME_TIMESTAMP
            source_realtime = log_entry.get('_SOURCE_REALTIME_TIMESTAMP')
            if source_realtime:
                return float(source_realtime) / 1_000_000
            
            # Last resort: parse timestamp from MESSAGE if available
            message = log_entry.get('MESSAGE', '')
            timestamp_match = re.search(r'(\w+\s+\d+\s+\d+:\d+:\d+)', message)
            if timestamp_match:
                try:
                    from datetime import datetime
                    # Try to parse timestamp like "Mai 31 07:08:13"
                    ts_str = timestamp_match.group(1)
                    # This is approximate since we don't have year/microseconds
                    dt = datetime.strptime(f"2024 {ts_str}", "%Y %b %d %H:%M:%S")
                    return dt.timestamp()
                except:
                    pass
                    
        except Exception as e:
            if self.verbose:
                print(f"Error extracting timestamp: {e}")
        
        return None

    def parse_systemd_logs(self, hours: int = 24) -> List[Dict]:
        """Parse systemd logs using journalctl for the specified service"""
        logs = []
        
        try:
            # Get systemd service name from config, default to backup.service
            service_name = "backup.service"
            if hasattr(self, 'influxdb_config') and self.influxdb_config:
                service_name = self.influxdb_config.get('metrics', {}).get('systemd_service', 'backup.service')
            
            # Calculate since timestamp
            since_time = datetime.now() - timedelta(hours=hours)
            since_str = since_time.strftime('%Y-%m-%d %H:%M:%S')
            
            if self.verbose:
                print(f"Querying journalctl for service '{service_name}' since {since_str}")
            
            # Run journalctl command to get logs in JSON format
            cmd = [
                'journalctl',
                '-u', service_name,
                '--since', since_str,
                '--output=json',
                '--no-pager'
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode != 0:
                if self.verbose:
                    print(f"journalctl command failed: {result.stderr}")
                return logs
            
            # Parse JSON log entries
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    try:
                        log_entry = json.loads(line)
                        logs.append(log_entry)
                    except json.JSONDecodeError as e:
                        if self.verbose:
                            print(f"Failed to parse log line as JSON: {e}")
                            
        except subprocess.TimeoutExpired:
            if self.verbose:
                print("journalctl command timed out")
        except Exception as e:
            if self.verbose:
                print(f"Error querying systemd logs: {e}")
        
        if self.verbose:
            print(f"Retrieved {len(logs)} log entries from systemd")
        
        return logs

    def extract_backup_sessions(self, logs: List[Dict]) -> List[Dict]:
        """Extract backup sessions in legacy format for backwards compatibility"""
        sessions = []
        current_session = None
        
        if self.verbose:
            print(f"[DEBUG] Extracting legacy backup sessions from {len(logs)} log entries")
        
        for log in logs:
            message = log.get('MESSAGE', '')
            
            # Handle ANSI escape sequences
            if isinstance(message, list):
                try:
                    message = bytes(message).decode('utf-8', errors='ignore')
                    message = re.sub(r'\x1b\[[0-9;]*m', '', message)
                except (ValueError, TypeError):
                    message = str(message)
            else:
                message = str(message)
                message = re.sub(r'\x1b\[[0-9;]*m', '', message)
            
            # Get timestamp
            timestamp = self._get_precise_timestamp(log)
            if not timestamp:
                continue
            
            # Look for run start markers
            run_start_match = re.search(r'BACKUP_RUN_START: run_id=(\w+)', message)
            if run_start_match:
                run_id = run_start_match.group(1)
                
                # Close previous session if exists
                if current_session:
                    current_session['end_time'] = datetime.fromtimestamp(timestamp)
                    current_session['duration'] = timestamp - current_session['start_timestamp']
                    sessions.append(current_session)
                
                # Start new session
                current_session = {
                    'run_id': run_id,
                    'start_time': datetime.fromtimestamp(timestamp),
                    'start_timestamp': timestamp,
                    'hosts': {},
                    'status': 'running',
                    'errors': [],
                    'warnings': []
                }
                
                if self.verbose:
                    print(f"[DEBUG] Started legacy session with run_id {run_id}")
                continue
            
            # Skip if no current session
            if not current_session:
                continue
            
            # Look for host backup starts
            host_start_match = re.search(r'=== Starting backup for (\w+) ===', message)
            if host_start_match:
                hostname = host_start_match.group(1)
                current_session['hosts'][hostname] = {
                    'status': 'running',
                    'start_time': datetime.fromtimestamp(timestamp),
                    'paths': [],
                    'errors': [],
                    'warnings': []
                }
                continue
            
            # Look for host completion
            host_completion_match = re.search(r'Host backup (completed successfully|completed with warnings|failed): (\w+)', message)
            if host_completion_match:
                completion_type = host_completion_match.group(1)
                hostname = host_completion_match.group(2)
                
                if hostname in current_session['hosts']:
                    if "successfully" in completion_type:
                        current_session['hosts'][hostname]['status'] = 'success'
                    elif "warnings" in completion_type:
                        current_session['hosts'][hostname]['status'] = 'warning'
                    else:
                        current_session['hosts'][hostname]['status'] = 'failed'
                    
                    current_session['hosts'][hostname]['end_time'] = datetime.fromtimestamp(timestamp)
                continue
            
            # Look for volume results
            volume_match = re.search(r'Volume backup (completed successfully|completed with warnings|failed): (\w+):(.+?)(?:\s+\(exit code: (\d+)\))?$', message)
            if volume_match:
                completion_type = volume_match.group(1)
                hostname = volume_match.group(2)
                path = volume_match.group(3).strip()
                exit_code = volume_match.group(4)
                
                if hostname in current_session['hosts']:
                    path_info = {
                        'path': path,
                        'timestamp': timestamp,
                        'status': 'success' if "successfully" in completion_type else 
                                 'warning' if "warnings" in completion_type else 'failed'
                    }
                    if exit_code:
                        path_info['exit_code'] = int(exit_code)
                    
                    current_session['hosts'][hostname]['paths'].append(path_info)
                continue
            
            # Look for errors and warnings
            if "ERROR" in message.upper() or "❌" in message:
                error_info = {
                    'timestamp': timestamp,
                    'message': message,
                    'type': 'error'
                }
                current_session['errors'].append(error_info)
            elif "WARNING" in message.upper() or "⚠" in message:
                warning_info = {
                    'timestamp': timestamp,
                    'message': message,
                    'type': 'warning'
                }
                current_session['warnings'].append(warning_info)
        
        # Handle incomplete session
        if current_session:
            current_session['end_time'] = datetime.now()
            current_session['duration'] = time.time() - current_session['start_timestamp']
            current_session['status'] = 'running'
            sessions.append(current_session)
            
            if self.verbose:
                print(f"[DEBUG] Added incomplete legacy session")
        
        # Determine overall session status based on host statuses
        for session in sessions:
            if session['status'] == 'running':
                continue
                
            host_statuses = [host_data.get('status', 'unknown') for host_data in session['hosts'].values()]
            
            if all(status == 'success' for status in host_statuses):
                session['status'] = 'success'
            elif any(status == 'failed' for status in host_statuses):
                session['status'] = 'failed'
            elif any(status == 'warning' for status in host_statuses):
                session['status'] = 'warning'
            else:
                session['status'] = 'unknown'
        
        if self.verbose:
            print(f"[DEBUG] Extracted {len(sessions)} legacy sessions")
        
        return sessions

    def extract_host_sessions_precise(self, logs: List[Dict]) -> List[Dict]:
        """Extract individual host backup sessions with precise timestamps"""
        host_sessions = []
        current_sessions = {}  # Track ongoing sessions by host
        
        if self.verbose:
            print(f"[DEBUG] Extracting precise host sessions from {len(logs)} log entries")
        
        for log in logs:
            message = log.get('MESSAGE', '')
            
            # Handle ANSI escape sequences
            if isinstance(message, list):
                try:
                    message = bytes(message).decode('utf-8', errors='ignore')
                    message = re.sub(r'\x1b\[[0-9;]*m', '', message)
                except (ValueError, TypeError):
                    message = str(message)
            else:
                message = str(message)
                message = re.sub(r'\x1b\[[0-9;]*m', '', message)
            
            # Get precise timestamp
            timestamp = self._get_precise_timestamp(log)
            if not timestamp:
                continue
            
            # Host backup start: "=== Starting backup for nasbox ==="
            start_match = re.search(r'=== Starting backup for (\w+) ===', message)
            if start_match:
                hostname = start_match.group(1)
                
                # Close any previous session for this host (shouldn't happen normally)
                if hostname in current_sessions:
                    old_session = current_sessions[hostname]
                    old_session['end_timestamp'] = timestamp
                    old_session['status'] = 'interrupted'
                    old_session['duration'] = timestamp - old_session['start_timestamp']
                    host_sessions.append(old_session)
                    if self.verbose:
                        print(f"[DEBUG] Closed interrupted session for {hostname}")
                
                # Start new session
                current_sessions[hostname] = {
                    'hostname': hostname,
                    'start_timestamp': timestamp,
                    'start_time': datetime.fromtimestamp(timestamp),
                    'status': 'running',
                    'paths': [],
                    'errors': [],
                    'warnings': []
                }
                
                if self.verbose:
                    print(f"[DEBUG] Started session for {hostname} at {timestamp}")
            
            # Host backup completion
            completion_match = re.search(r'(✓|⚠|❌)?\s*Host backup (completed successfully|completed with warnings|failed): (\w+)', message)
            if completion_match:
                status_symbol = completion_match.group(1) or ''
                completion_type = completion_match.group(2)
                hostname = completion_match.group(3)
                
                if hostname in current_sessions:
                    session = current_sessions[hostname]
                    session['end_timestamp'] = timestamp
                    session['end_time'] = datetime.fromtimestamp(timestamp)
                    session['duration'] = timestamp - session['start_timestamp']
                    
                    # Determine status
                    if "successfully" in completion_type or "✓" in status_symbol:
                        session['status'] = 'success'
                    elif "warnings" in completion_type or "⚠" in status_symbol:
                        session['status'] = 'warning'
                    else:
                        session['status'] = 'failed'
                    
                    host_sessions.append(session)
                    del current_sessions[hostname]
                    
                    if self.verbose:
                        print(f"[DEBUG] Completed session for {hostname}: {session['status']} (duration: {session['duration']:.1f}s)")
            
            # Volume backup results
            volume_match = re.search(r'(✓|⚠|❌)?\s*Volume backup (completed successfully|completed with warnings|failed): (\w+):(.+?)(?:\s+\(exit code: (\d+)\))?$', message)
            if volume_match:
                status_symbol = volume_match.group(1) or ''
                completion_type = volume_match.group(2)
                hostname = volume_match.group(3)
                path = volume_match.group(4).strip()
                exit_code = volume_match.group(5)
                
                if hostname in current_sessions:
                    # Determine status
                    if "successfully" in completion_type or "✓" in status_symbol:
                        status = 'success'
                    elif "warnings" in completion_type or "⚠" in status_symbol:
                        status = 'warning'
                    else:
                        status = 'failed'
                    
                    path_info = {
                        'path': path,
                        'timestamp': timestamp,
                        'status': status
                    }
                    
                    if exit_code:
                        path_info['exit_code'] = int(exit_code)
                    
                    current_sessions[hostname]['paths'].append(path_info)
            
            # Error detection
            if any(hostname in current_sessions for hostname in current_sessions):
                if "rsync:" in message and ("error" in message.lower() or "failed" in message.lower()):
                    error_info = {
                        'timestamp': timestamp,
                        'message': message,
                        'type': 'rsync_error'
                    }
                    
                    # Add to all running sessions (we might not know which host this belongs to)
                    for session in current_sessions.values():
                        session['errors'].append(error_info)
                
                # Warning detection
                elif "⚠" in message or "WARNING" in message.upper():
                    warning_info = {
                        'timestamp': timestamp,
                        'message': message,
                        'type': 'warning'
                    }
                    
                    for session in current_sessions.values():
                        session['warnings'].append(warning_info)
        
        # Handle incomplete sessions (still running)
        for hostname, session in current_sessions.items():
            session['end_timestamp'] = time.time()
            session['end_time'] = datetime.now()
            session['status'] = 'running'
            session['duration'] = session['end_timestamp'] - session['start_timestamp']
            host_sessions.append(session)
            
            if self.verbose:
                print(f"[DEBUG] Added incomplete session for {hostname}: {session['status']}")
        
        if self.verbose:
            print(f"[DEBUG] Extracted {len(host_sessions)} host sessions")
        
        return host_sessions

    def _generate_session_marker(self, hostname: str, start_timestamp: float, end_timestamp: float) -> str:
        """Generate a unique marker for a host session based on timestamps"""
        # Use microsecond precision for uniqueness
        start_us = int(start_timestamp * 1_000_000)
        end_us = int(end_timestamp * 1_000_000)
        return f"{hostname}_{start_us}_{end_us}"

    def _is_session_already_sent(self, hostname: str, start_timestamp: float, end_timestamp: float) -> bool:
        """Check if a specific host session has already been sent to InfluxDB"""
        marker = self._generate_session_marker(hostname, start_timestamp, end_timestamp)
        return marker in self.sent_timestamps

    def _mark_session_as_sent(self, hostname: str, start_timestamp: float, end_timestamp: float):
        """Mark a specific host session as sent to InfluxDB"""
        marker = self._generate_session_marker(hostname, start_timestamp, end_timestamp)
        self.sent_timestamps.add(marker)

    def send_to_influxdb(self, host: str = None, port: int = None, database: str = None, 
                        username: str = None, password: str = None, ssl: bool = None,
                        token: str = None, organization: str = None, bucket: str = None):
        """Send metrics directly to InfluxDB via HTTP API with precise timestamp-based deduplication"""
        try:
            import requests
        except ImportError:
            print("Error: requests library not found. Install with: pip install requests")
            return False

        # Load existing timestamp state
        self.sent_timestamps = self._load_sent_timestamps_state()
        
        if self.verbose:
            print(f"Loaded {len(self.sent_timestamps)} previously sent session markers")

        influx_cfg = self.influxdb_config.get('influxdb', {})
        host = host or influx_cfg.get('host', 'localhost')
        port = port or influx_cfg.get('port', 8086)
        ssl = ssl if ssl is not None else influx_cfg.get('ssl', False)
        
        token = str(token or influx_cfg.get('token', '')).strip()
        organization = str(organization or influx_cfg.get('organization', '')).strip()
        bucket = str(bucket or influx_cfg.get('bucket', '')).strip()
        
        if not organization and token:
            organization = 'home'
        
        database = database or influx_cfg.get('database', 'backup_metrics')
        username = username or influx_cfg.get('username', '')
        password = password or influx_cfg.get('password', '')
        
        is_influxdb_2x = bool(token and organization)
        
        if is_influxdb_2x:
            if not bucket:
                print("Error: InfluxDB 2.x requires bucket to be configured")
                return False
            print(f"Sending metrics to InfluxDB 2.x at {host}:{port}, organization: {organization}, bucket: {bucket}")
        else:
            print(f"Sending metrics to InfluxDB 1.x at {host}:{port}, database: {database}")
        
        # Get host sessions with precise timestamps
        recent_logs = self.parse_systemd_logs(hours=48)  # Look back further for completeness
        host_sessions = self.extract_host_sessions_precise(recent_logs)
        
        # Filter out sessions we've already sent
        new_sessions = []
        duplicate_count = 0
        
        for session in host_sessions:
            hostname = session['hostname']
            start_ts = session['start_timestamp']
            end_ts = session.get('end_timestamp', time.time())
            
            if not self._is_session_already_sent(hostname, start_ts, end_ts):
                new_sessions.append(session)
            else:
                duplicate_count += 1
                if self.verbose:
                    start_time = datetime.fromtimestamp(start_ts).strftime('%Y-%m-%d %H:%M:%S')
                    print(f"Skipping duplicate session: {hostname} at {start_time}")
        
        if not new_sessions:
            print("✓ No new host sessions to send to InfluxDB (all sessions already sent)")
            return True
            
        print(f"Sending {len(new_sessions)} new host sessions to InfluxDB...")
        if duplicate_count > 0:
            print(f"Filtered out {duplicate_count} duplicate sessions")
        
        # Generate InfluxDB line protocol with precise timestamps
        lines = []
        protocol = "https" if ssl else "http"
        
        for session in new_sessions:
            hostname = session['hostname']
            start_ts = session['start_timestamp']
            end_ts = session.get('end_timestamp', time.time())
            
            # Convert to nanoseconds for InfluxDB
            session_timestamp_ns = int(start_ts * 1_000_000_000)
            
            # Session-level metrics using backup_host_session measurement
            status = session.get('status', 'unknown')
            status_numeric = 1.0 if status == 'success' else 0.5 if status == 'warning' else 0.0
            duration = session.get('duration', 0)
            error_count = len(session.get('errors', []))
            warning_count = len(session.get('warnings', []))
            path_count = len(session.get('paths', []))
            
            session_line = (
                f"backup_host_session,host={hostname} "
                f"status_numeric={status_numeric},"
                f"duration_seconds={float(duration)},"
                f"error_count={error_count}i,"
                f"warning_count={warning_count}i,"
                f"path_count={path_count}i,"
                f"start_timestamp={start_ts},"
                f"end_timestamp={end_ts} "
                f"{session_timestamp_ns}"
            )
            lines.append(session_line)
            
            # Host status summary using backup_host_status measurement
            # Get latest error and warning messages
            last_error = ""
            last_warning = ""
            if session.get('errors'):
                last_error = session['errors'][-1]['message'].replace('"', '\\"')
            if session.get('warnings'):
                last_warning = session['warnings'][-1]['message'].replace('"', '\\"')
            
            host_status_line = (
                f"backup_host_status,host={hostname} "
                f"status_numeric={status_numeric},"
                f"error_count={error_count}i,"
                f"warning_count={warning_count}i,"
                f"volume_count={path_count}i,"
                f'last_error="{last_error}",'
                f'last_warning="{last_warning}" '
                f"{session_timestamp_ns}"
            )
            lines.append(host_status_line)
            
            # Volume-level metrics using backup_volume_session and backup_volume_status
            for path_detail in session.get('paths', []):
                path = path_detail.get('path', 'unknown')
                path_escaped = path.replace(' ', '\\ ').replace(',', '\\,').replace('=', '\\=')
                
                volume_status_numeric = 1.0 if path_detail['status'] == 'success' else 0.5 if path_detail['status'] == 'warning' else 0.0
                exit_code = path_detail.get('exit_code', 0)
                
                # Use volume's precise timestamp
                volume_timestamp_ns = int(path_detail['timestamp'] * 1_000_000_000)
                
                # backup_volume_session measurement
                volume_session_line = (
                    f"backup_volume_session,host={hostname},path={path_escaped} "
                    f"status_numeric={volume_status_numeric},"
                    f"exit_code={exit_code}i,"
                    f"session_start_timestamp={start_ts} "
                    f"{volume_timestamp_ns}"
                )
                lines.append(volume_session_line)
                
                # backup_volume_status measurement
                volume_status_line = (
                    f"backup_volume_status,host={hostname},path={path_escaped} "
                    f"status_numeric={volume_status_numeric},"
                    f"exit_code={exit_code}i "
                    f"{volume_timestamp_ns}"
                )
                lines.append(volume_status_line)
        
        # Add current directory size data with current timestamp
        current_timestamp_ns = int(time.time() * 1_000_000_000)
        dir_stats = self.get_backup_directory_stats_fast()
        
        for hostname, host_data in dir_stats.items():
            size_bytes = host_data.get('size_bytes', 0)
            file_count = host_data.get('file_count', 0)
            scan_complete = 1 if hostname in self.du_results else 0
            
            host_size_line = (
                f"backup_host_size,host={hostname} "
                f"size_bytes={size_bytes}i,"
                f"file_count={file_count}i,"
                f"scan_complete={scan_complete}i "
                f"{current_timestamp_ns}"
            )
            lines.append(host_size_line)
        
        # Add overall backup status if we have sessions
        if new_sessions:
            # Calculate overall metrics
            total_sessions = len(new_sessions)
            successful_sessions = len([s for s in new_sessions if s.get('status') == 'success'])
            warning_sessions = len([s for s in new_sessions if s.get('status') == 'warning'])
            failed_sessions = len([s for s in new_sessions if s.get('status') == 'failed'])
            
            # Overall status: success if all successful, warning if any warnings, failed if any failed
            if failed_sessions > 0:
                overall_status_numeric = 0.0
            elif warning_sessions > 0:
                overall_status_numeric = 0.5
            else:
                overall_status_numeric = 1.0
            
            # Get total duration and errors across all sessions
            total_duration = sum(s.get('duration', 0) for s in new_sessions)
            total_errors = sum(len(s.get('errors', [])) for s in new_sessions)
            total_size = sum(dir_stats.get(s['hostname'], {}).get('size_bytes', 0) for s in new_sessions)
            
            # Generate a session ID based on the timestamp of the latest session
            latest_session = max(new_sessions, key=lambda x: x['start_timestamp'])
            session_id = hashlib.md5(str(latest_session['start_timestamp']).encode()).hexdigest()[:8]
            
            backup_status_line = (
                f"backup_status "
                f"status_numeric={overall_status_numeric},"
                f"duration_seconds={float(total_duration)},"
                f"error_count={total_errors}i,"
                f"total_size_bytes={total_size}i,"
                f'session_id="{session_id}" '
                f"{current_timestamp_ns}"
            )
            lines.append(backup_status_line)

        if not lines:
            print("✓ No data to send to InfluxDB")
            return True

        if self.verbose:
            print(f"Generated {len(lines)} metrics lines:")
            for line in lines[:3]:
                print(f"  {line}")
            if len(lines) > 3:
                print(f"  ... and {len(lines) - 3} more")

        # Send to InfluxDB
        data = '\n'.join(lines)

        if is_influxdb_2x:
            url = f"{protocol}://{host}:{port}/api/v2/write"
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'text/plain; charset=utf-8'
            }
            params = {
                'org': organization,
                'bucket': bucket,
                'precision': 'ns'
            }
        else:
            url = f"{protocol}://{host}:{port}/write"
            headers = {'Content-Type': 'application/octet-stream'}
            params = {"db": database}
            if username and password:
                params["u"] = username
                params["p"] = password
        
        try:
            response = requests.post(url, params=params, data=data, headers=headers, timeout=30)
            
            if response.status_code == 204:
                influx_version = "2.x" if is_influxdb_2x else "1.x"
                print(f"✅ Successfully sent {len(new_sessions)} new host sessions to InfluxDB {influx_version}")
                
                # Mark all sent sessions
                for session in new_sessions:
                    hostname = session['hostname']
                    start_ts = session['start_timestamp']
                    end_ts = session.get('end_timestamp', time.time())
                    self._mark_session_as_sent(hostname, start_ts, end_ts)
                
                # Save state after successful send
                self._save_sent_timestamps_state()
                
                return True
            else:
                print(f"❌ Error sending to InfluxDB: HTTP {response.status_code} - {response.text}")
                return False
                
        except requests.exceptions.RequestException as e:
            print(f"❌ Error connecting to InfluxDB: {e}")
            return False

    def reset_sent_data_state(self):
        """Reset the sent data state - useful for development/testing"""
        try:
            if Path(self.state_file).exists():
                Path(self.state_file).unlink()
            self.sent_timestamps = set()
            print("✅ Reset sent timestamp state - next run will send all available data")
        except Exception as e:
            print(f"❌ Error resetting state: {e}")

    def get_backup_directory_stats_fast(self, backup_base: str = "/share/backup") -> Dict:
        """Get quick filesystem statistics using cached data and basic stats"""
        print(f"Getting quick backup directory stats from {backup_base}...")
        stats = {}
        backup_path = Path(backup_base)
        
        if not backup_path.exists():
            print(f"Backup path {backup_base} does not exist")
            return stats
        
        host_dirs = [d for d in backup_path.iterdir() if d.is_dir()]
        print(f"Found {len(host_dirs)} host directories")
        
        # First, get any cached results from background scan
        while not self.du_queue.empty():
            try:
                hostname, host_stats = self.du_queue.get_nowait()
                self.du_results[hostname] = host_stats
            except queue.Empty:
                break
        
        # For directories we have cached data, use it
        for hostname, cached_stats in self.du_results.items():
            stats[hostname] = cached_stats
        
        # For directories we don't have cached data, use quick stats
        for host_dir in host_dirs:
            hostname = host_dir.name
            if hostname not in stats:
                try:
                    stat_info = host_dir.stat()
                    stats[hostname] = {
                        'size_bytes': 0,  # Will be updated by background scan
                        'file_count': 0,  # Will be updated by background scan
                        'last_modified': datetime.fromtimestamp(stat_info.st_mtime),
                        'path': str(host_dir),
                        'scan_duration': 0,
                        'scan_method': 'quick_stat'
                    }
                except Exception as e:
                    if self.verbose:
                        print(f"Error getting quick stats for {hostname}: {e}")
                    stats[hostname] = {
                        'size_bytes': 0,
                        'file_count': 0,
                        'last_modified': None,
                        'path': str(host_dir),
                        'scan_duration': 0,
                        'scan_method': 'error'
                    }
        
        return stats

    def start_background_directory_scan(self, backup_base: str = "/share/backup"):
        """Start background thread to scan directory sizes"""
        backup_path = Path(backup_base)
        
        if not backup_path.exists():
            print(f"Backup path {backup_base} does not exist")
            return
        
        host_dirs = [d for d in backup_path.iterdir() if d.is_dir()]
        
        # Only start scan if we don't have one running
        if self.du_thread is None or not self.du_thread.is_alive():
            self.du_completed.clear()
            self.du_thread = threading.Thread(
                target=self._directory_scanner_worker,
                args=(host_dirs, backup_base),
                daemon=True
            )
            self.du_thread.start()
            print(f"Started background directory scan thread for {len(host_dirs)} directories")
        else:
            print("Background directory scan already running")

    def generate_metrics_fast(self, hours: int = 24) -> Dict:
        """Generate metrics quickly without waiting for du to complete - now uses same logic as InfluxDB"""
        logs = self.parse_systemd_logs(hours=hours)
        
        # Use the same precise session extraction as InfluxDB
        host_sessions = self.extract_host_sessions_precise(logs)
        
        # Legacy session format for backwards compatibility
        sessions = self.extract_backup_sessions(logs)
        
        dir_stats = self.get_backup_directory_stats_fast()
        rsync_stats = self.parse_rsync_statistics(hours=hours)
        
        # Get latest session from either format
        latest_session = sessions[-1] if sessions else None
        latest_host_session = host_sessions[-1] if host_sessions else None
        
        # Calculate host status using precise host sessions
        host_status = {}
        for hostname in self.hosts:
            host_status[hostname] = {
                'last_backup_success': False,
                'last_backup_status': 'unknown',
                'last_backup_time': None,
                'last_error': None,
                'last_warning': None,
                'total_size_bytes': dir_stats.get(hostname, {}).get('size_bytes', 0),
                'file_count': dir_stats.get(hostname, {}).get('file_count', 0),
                'last_modified': dir_stats.get(hostname, {}).get('last_modified'),
                'scan_complete': hostname in self.du_results,
                'path_details': [],
                'status_numeric': 0,  # 0=failed, 0.5=warning, 1=success
                'session_data': None  # New field for precise session data
            }
            
            # Find the most recent session for this host using precise tracking
            host_latest_session = None
            for session in reversed(host_sessions):
                if session['hostname'] == hostname:
                    host_latest_session = session
                    break
            
            if host_latest_session:
                status = host_latest_session.get('status', 'unknown')
                host_status[hostname]['last_backup_status'] = status
                host_status[hostname]['last_backup_success'] = status in ['success', 'warning']
                host_status[hostname]['last_backup_time'] = host_latest_session['start_time']
                
                # Set numeric status for easier dashboard visualization
                if status == 'success':
                    host_status[hostname]['status_numeric'] = 1.0
                elif status == 'warning':
                    host_status[hostname]['status_numeric'] = 0.5
                else:
                    host_status[hostname]['status_numeric'] = 0.0
                
                # Add precise session data
                host_status[hostname]['session_data'] = {
                    'start_timestamp': host_latest_session['start_timestamp'],
                    'end_timestamp': host_latest_session.get('end_timestamp'),
                    'duration': host_latest_session.get('duration', 0),
                    'error_count': len(host_latest_session.get('errors', [])),
                    'warning_count': len(host_latest_session.get('warnings', [])),
                    'path_count': len(host_latest_session.get('paths', []))
                }
                
                # Get path details with status and rsync statistics
                path_details = []
                for path_info in host_latest_session.get('paths', []):
                    path_detail = {
                        'path': path_info['path'],
                        'status': path_info['status'],
                        'timestamp': path_info['timestamp'],
                        'status_numeric': 1.0 if path_info['status'] == 'success' else 
                                        0.5 if path_info['status'] == 'warning' else 0.0
                    }
                    if 'exit_code' in path_info:
                        path_detail['exit_code'] = path_info['exit_code']
                    
                    # Add rsync statistics if available
                    if hostname in rsync_stats and path_info['path'] in rsync_stats[hostname]:
                        rsync_data = rsync_stats[hostname][path_info['path']]
                        path_detail.update({
                            'bytes_sent': rsync_data.get('bytes_sent', 0),
                            'bytes_received': rsync_data.get('bytes_received', 0),
                            'transfer_rate': rsync_data.get('transfer_rate', 0),
                            'total_size': rsync_data.get('total_size', 0),
                            'speedup': rsync_data.get('speedup', 0.0)
                        })
                    
                    path_details.append(path_detail)
                
                host_status[hostname]['path_details'] = path_details
                
                # Get latest error and warning
                if host_latest_session.get('errors'):
                    host_status[hostname]['last_error'] = host_latest_session['errors'][-1]['message']
                if host_latest_session.get('warnings'):
                    host_status[hostname]['last_warning'] = host_latest_session['warnings'][-1]['message']
            
            # Fallback to legacy session format if no precise session found
            elif latest_session and hostname in latest_session.get('hosts', {}):
                host_data = latest_session['hosts'][hostname]
                status = host_data.get('status', 'unknown')
                host_status[hostname]['last_backup_status'] = status
                host_status[hostname]['last_backup_success'] = status in ['success', 'warning']
                host_status[hostname]['last_backup_time'] = latest_session['start_time']
                
                # Set numeric status for easier dashboard visualization
                if status == 'success':
                    host_status[hostname]['status_numeric'] = 1.0
                elif status == 'warning':
                    host_status[hostname]['status_numeric'] = 0.5
                else:
                    host_status[hostname]['status_numeric'] = 0.0
                
                # Legacy path details
                path_details = []
                for path_info in host_data.get('paths', []):
                    path_detail = {
                        'path': path_info['path'],
                        'status': path_info['status'],
                        'timestamp': path_info['timestamp'],
                        'status_numeric': 1.0 if path_info['status'] == 'success' else 
                                        0.5 if path_info['status'] == 'warning' else 0.0
                    }
                    if 'exit_code' in path_info:
                        path_detail['exit_code'] = path_info['exit_code']
                    
                    # Add rsync statistics if available
                    if hostname in rsync_stats and path_info['path'] in rsync_stats[hostname]:
                        rsync_data = rsync_stats[hostname][path_info['path']]
                        path_detail.update({
                            'bytes_sent': rsync_data.get('bytes_sent', 0),
                            'bytes_received': rsync_data.get('bytes_received', 0),
                            'transfer_rate': rsync_data.get('transfer_rate', 0),
                            'total_size': rsync_data.get('total_size', 0),
                            'speedup': rsync_data.get('speedup', 0.0)
                        })
                    
                    path_details.append(path_detail)
                
                host_status[hostname]['path_details'] = path_details
                
                # Get latest error and warning
                if host_data.get('errors'):
                    host_status[hostname]['last_error'] = host_data['errors'][-1]['message']
                if host_data.get('warnings'):
                    host_status[hostname]['last_warning'] = host_data['warnings'][-1]['message']
        
        # Create summary from host sessions matching InfluxDB data structure
        total_new_sessions = len(host_sessions)
        recent_sessions = sorted(host_sessions, key=lambda x: x['start_timestamp'], reverse=True)[:10]
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'total_sessions': len(sessions),  # Legacy session count
            'total_host_sessions': total_new_sessions,  # New precise session count
            'background_scan_active': self.du_thread is not None and self.du_thread.is_alive(),
            'background_scan_complete': self.du_completed.is_set(),
            'scanned_hosts': len(self.du_results),
            'total_hosts': len(self.hosts),
            'latest_session': {
                'status': latest_session['status'] if latest_session else 'unknown',
                'start_time': latest_session['start_time'].isoformat() if latest_session else None,
                'end_time': latest_session.get('end_time').isoformat() if latest_session and latest_session.get('end_time') else None,
                'duration_seconds': int(latest_session.get('duration', 0)) if latest_session else 0,
                'error_count': len(latest_session.get('errors', [])) if latest_session else 0,
                'hosts_attempted': len(latest_session.get('hosts', {})) if latest_session else 0,
                'hosts_successful': len([h for h in latest_session.get('hosts', {}).values() if h.get('status') != 'failed']) if latest_session else 0
            },
            'latest_host_session': {
                'hostname': latest_host_session['hostname'] if latest_host_session else None,
                'status': latest_host_session.get('status', 'unknown') if latest_host_session else 'unknown',
                'start_time': latest_host_session['start_time'].isoformat() if latest_host_session else None,
                'end_time': latest_host_session.get('end_time').isoformat() if latest_host_session and latest_host_session.get('end_time') else None,
                'duration_seconds': latest_host_session.get('duration', 0) if latest_host_session else 0,
                'error_count': len(latest_host_session.get('errors', [])) if latest_host_session else 0,
                'warning_count': len(latest_host_session.get('warnings', [])) if latest_host_session else 0,
                'path_count': len(latest_host_session.get('paths', [])) if latest_host_session else 0
            },
            'hosts': host_status,
            'total_backup_size_bytes': sum(h.get('total_size_bytes', 0) for h in host_status.values()),
            'total_files': sum(h.get('file_count', 0) for h in host_status.values()),
            'sessions': sessions[-10:],  # Last 10 legacy sessions for backwards compatibility
            'host_sessions': [
                {
                    'hostname': s['hostname'],
                    'start_time': s['start_time'].isoformat(),
                    'end_time': s.get('end_time').isoformat() if s.get('end_time') else None,
                    'status': s.get('status', 'unknown'),
                    'duration': s.get('duration', 0),
                    'error_count': len(s.get('errors', [])),
                    'warning_count': len(s.get('warnings', [])),
                    'path_count': len(s.get('paths', [])),
                    'start_timestamp': s['start_timestamp'],
                    'end_timestamp': s.get('end_timestamp')
                } for s in recent_sessions
            ],  # Last 10 precise host sessions matching InfluxDB structure
            'deduplication_info': {
                'sent_session_count': len(self.sent_timestamps),
                'would_send_new_sessions': len([s for s in host_sessions if not self._is_session_already_sent(
                    s['hostname'], s['start_timestamp'], s.get('end_timestamp', time.time())
                )])
            }
        }
        
        return metrics

    def generate_metrics(self, hours: int = 24) -> Dict:
        """Original generate_metrics method - kept for compatibility"""
        return self.generate_metrics_fast(hours)

    def test_influxdb_connection(self):
        """Test InfluxDB connection and authentication"""
        try:
            import requests
        except ImportError:
            print("Error: requests library not found. Install with: pip install requests")
            return False
        
        influx_cfg = self.influxdb_config.get('influxdb', {})
        host = influx_cfg.get('host', 'localhost')
        port = influx_cfg.get('port', 8086)
        ssl = influx_cfg.get('ssl', False)
        
        # InfluxDB 2.x support - handle both string and None values
        token = str(influx_cfg.get('token', '')).strip()
        organization = str(influx_cfg.get('organization', '')).strip()
        bucket = str(influx_cfg.get('bucket', '')).strip()
        
        # Manual fix for organization field - check if it's empty and try to use 'home' as default
        if not organization and token:
            print("Warning: Organization field is empty, using 'home' as default")
            organization = 'home'
        
        # InfluxDB 1.x support
        database = influx_cfg.get('database', 'backup_metrics')
        username = influx_cfg.get('username', '')
        password = influx_cfg.get('password', '')
        
        # Determine InfluxDB version - check for both token and organization
        is_influxdb_2x = bool(token and organization)
        
        print(f"Testing InfluxDB connection to {host}:{port}...")
        
        if is_influxdb_2x:
            if not bucket:
                print("❌ Error: InfluxDB 2.x requires bucket to be configured")
                return False
            print(f"   Mode: InfluxDB 2.x (Token auth)")
            print(f"   Organization: {organization}")
            print(f"   Bucket: {bucket}")
        else:
            if token and not organization:
                print("❌ Error: InfluxDB 2.x token provided but organization is missing")
                print("   Please check your YAML file and ensure the organization field is properly set")
                print("   Your config shows: organization: ''")
                print("   It should be: organization: 'home' or organization: home")
                return False
            elif not token and organization:
                print("❌ Error: InfluxDB 2.x organization provided but token is missing")
                return False
            print(f"   Mode: InfluxDB 1.x (Username/Password auth)")
            print(f"   Database: {database}")
            if username:
                print(f"   Username: {username}")
        
        protocol = "https" if ssl else "http"
        
        # Test different endpoints based on version
        if is_influxdb_2x:
            # Test InfluxDB 2.x health endpoint
            health_url = f"{protocol}://{host}:{port}/health"
            try:
                print("   Testing health endpoint...", end=" ")
                response = requests.get(health_url, timeout=10)
                if response.status_code == 200:
                    print("✓")
                else:
                    print(f"⚠ (HTTP {response.status_code})")
            except Exception as e:
                print(f"❌ ({e})")
                return False
            
            # Test authentication with ready endpoint
            ready_url = f"{protocol}://{host}:{port}/ready"
            headers = {'Authorization': f'Token {token}'}
            try:
                print("   Testing authentication...", end=" ")
                response = requests.get(ready_url, headers=headers, timeout=10)
                if response.status_code == 200:
                    print("✓")
                else:
                    print(f"❌ (HTTP {response.status_code} - {response.text})")
                    return False
            except Exception as e:
                print(f"❌ ({e})")
                return False
            
            # Test bucket access with a query
            query_url = f"{protocol}://{host}:{port}/api/v2/query"
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'application/vnd.flux'
            }
            params = {'org': organization}
            # Simple flux query to test bucket access
            flux_query = f'buckets() |> filter(fn: (r) => r.name == "{bucket}") |> limit(n:1)'
            
            try:
                print("   Testing bucket access...", end=" ")
                response = requests.post(query_url, params=params, headers=headers, 
                                       data=flux_query, timeout=10)
                if response.status_code == 200:
                    print("✓")
                else:
                    print(f"❌ (HTTP {response.status_code} - {response.text})")
                    return False
            except Exception as e:
                print(f"❌ ({e})")
                return False
                
        else:
            # Test InfluxDB 1.x ping endpoint
            ping_url = f"{protocol}://{host}:{port}/ping"
            try:
                print("   Testing ping endpoint...", end=" ")
                response = requests.get(ping_url, timeout=10)
                if response.status_code == 204:
                    print("✓")
                else:
                    print(f"⚠ (HTTP {response.status_code})")
            except Exception as e:
                print(f"❌ ({e})")
                return False
            
            # Test database query
            query_url = f"{protocol}://{host}:{port}/query"
            params = {
                "q": "SHOW DATABASES",
                "db": database
            }
            if username and password:
                params["u"] = username
                params["p"] = password
            
            try:
                print("   Testing database access...", end=" ")
                response = requests.get(query_url, params=params, timeout=10)
                if response.status_code == 200:
                    result = response.json()
                    if 'results' in result and result['results']:
                        print("✓")
                    else:
                        print(f"❌ (Invalid response format)")
                        return False
                else:
                    print(f"❌ (HTTP {response.status_code} - {response.text})")
                    return False
            except Exception as e:
                print(f"❌ ({e})")
                return False
        
        print("✅ InfluxDB connection test successful!")
        return True

    def delete_all_backup_data(self):
        """Delete all backup data from InfluxDB - for development use only"""
        try:
            import requests
        except ImportError:
            print("Error: requests library not found. Install with: pip install requests")
            return False
        
        influx_cfg = self.influxdb_config.get('influxdb', {})
        host = influx_cfg.get('host', 'localhost')
        port = influx_cfg.get('port', 8086)
        ssl = influx_cfg.get('ssl', False)
        
        # InfluxDB 2.x support
        token = str(influx_cfg.get('token', '')).strip()
        organization = str(influx_cfg.get('organization', '')).strip()
        bucket = str(influx_cfg.get('bucket', '')).strip()
        
        if not organization and token:
            organization = 'home'
        
        # InfluxDB 1.x support
        database = influx_cfg.get('database', 'backup_metrics')
        username = influx_cfg.get('username', '')
        password = influx_cfg.get('password', '')
        
        is_influxdb_2x = bool(token and organization)
        protocol = "https" if ssl else "http"
        
        print("🗑️  WARNING: This will delete ALL backup data from InfluxDB!")
        print(f"Target: {host}:{port}")
        if is_influxdb_2x:
            print(f"Organization: {organization}, Bucket: {bucket}")
        else:
            print(f"Database: {database}")
        
        # Ask for confirmation
        confirm = input("Type 'DELETE' to confirm deletion of all backup data: ")
        if confirm != "DELETE":
            print("Deletion cancelled")
            return False
        
        if is_influxdb_2x:
            # InfluxDB 2.x - Delete measurements one by one
            measurements = [
                "backup_status",
                "backup_session_history", 
                "backup_host_history",
                "backup_host_size",
                "backup_volume_history",
                "backup_host_status",
                "backup_volume_status"
            ]
            
            for measurement in measurements:
                print(f"Deleting measurement: {measurement}")
                
                # Use delete API with predicate to delete all data
                delete_url = f"{protocol}://{host}:{port}/api/v2/delete"
                headers = {
                    'Authorization': f'Token {token}',
                    'Content-Type': 'application/json'
                }
                
                # Delete all data from this measurement
                delete_data = {
                    "start": "1970-01-01T00:00:00Z",
                    "stop": "2099-12-31T23:59:59Z",
                    "predicate": f'_measurement="{measurement}"'
                }
                
                try:
                    response = requests.post(
                        delete_url, 
                        params={'org': organization, 'bucket': bucket},
                        headers=headers,
                        json=delete_data,
                        timeout=30
                    )
                    
                    if response.status_code == 204:
                        print(f"  ✓ Deleted {measurement}")
                    else:
                        print(f"  ⚠ Error deleting {measurement}: HTTP {response.status_code} - {response.text}")
                        
                except Exception as e:
                    print(f"  ❌ Error deleting {measurement}: {e}")
        else:
            # InfluxDB 1.x - Drop and recreate database
            print(f"Dropping and recreating database: {database}")
            
            drop_url = f"{protocol}://{host}:{port}/query"
            params = {
                'q': f'DROP DATABASE "{database}"'
            }
            
            if username and password:
                params['u'] = username
                params['p'] = password
            
            try:
                # Drop database
                response = requests.post(drop_url, params=params, timeout=30)
                if response.status_code == 200:
                    print(f"  ✓ Dropped database {database}")
                else:
                    print(f"  ⚠ Error dropping database: HTTP {response.status_code} - {response.text}")
                
                # Recreate database
                params['q'] = f'CREATE DATABASE "{database}"'
                response = requests.post(drop_url, params=params, timeout=30)
                if response.status_code == 200:
                    print(f"  ✓ Recreated database {database}")
                else:
                    print(f"  ⚠ Error recreating database: HTTP {response.status_code} - {response.text}")
                    
            except Exception as e:
                print(f"  ❌ Error with database operations: {e}")
                return False
        
        # Also reset the sent data state
        self.reset_sent_data_state()
        
        print("✅ All backup data has been deleted from InfluxDB")
        print("💡 Next run of --send-influxdb will send all available data")
        
        return True

    def query_influxdb_data(self, hours: int = 24):
        """Query and display existing data from InfluxDB"""
        try:
            import requests
        except ImportError:
            print("Error: requests library not found. Install with: pip install requests")
            return False
        
        influx_cfg = self.influxdb_config.get('influxdb', {})
        influx_host = influx_cfg.get('host', 'localhost')
        influx_port = influx_cfg.get('port', 8086)
        ssl = influx_cfg.get('ssl', False)
        
        # InfluxDB 2.x support
        token = str(influx_cfg.get('token', '')).strip()
        organization = str(influx_cfg.get('organization', '')).strip()
        bucket = str(influx_cfg.get('bucket', '')).strip()
        
        if not organization and token:
            organization = 'home'
        
        # InfluxDB 1.x support
        database = influx_cfg.get('database', 'backup_metrics')
        username = influx_cfg.get('username', '')
        password = influx_cfg.get('password', '')
        
        is_influxdb_2x = bool(token and organization)
        protocol = "https" if ssl else "http"
        
        if is_influxdb_2x:
            print(f"Querying InfluxDB 2.x data from {influx_host}:{influx_port} (last {hours} hours)")
            print(f"Organization: {organization}, Bucket: {bucket}")
            
            # Enhanced queries that specifically look for error messages
            queries = {
                "Backup Status": f'from(bucket: "{bucket}") |> range(start: -{hours}h) |> filter(fn: (r) => r._measurement == "backup_status") |> last()',
                "Host Status (Basic)": f'from(bucket: "{bucket}") |> range(start: -{hours}h) |> filter(fn: (r) => r._measurement == "backup_host_status" and (r._field == "status_numeric" or r._field == "error_count" or r._field == "volume_count")) |> last()',
                "Error Messages": f'from(bucket: "{bucket}") |> range(start: -{hours}h) |> filter(fn: (r) => r._measurement == "backup_host_status" and (r._field == "last_error" or r._field == "last_warning")) |> filter(fn: (r) => r._value != "") |> last()',
                "Volume Status (Errors)": f'from(bucket: "{bucket}") |> range(start: -{hours}h) |> filter(fn: (r) => r._measurement == "backup_volume_status" and (r._field == "exit_code" or r._field == "status_numeric")) |> filter(fn: (r) => r._value != 0 and r._value != 1.0) |> last()',
                "Recent Host Sizes": f'from(bucket: "{bucket}") |> range(start: -{hours}h) |> filter(fn: (r) => r._measurement == "backup_host_size") |> last()'
            }
            
            for query_name, flux_query in queries.items():
                print(f"\n=== {query_name} ===")
                url = f"{protocol}://{influx_host}:{influx_port}/api/v2/query"
                headers = {
                    'Authorization': f'Token {token}',
                    'Content-Type': 'application/vnd.flux'
                }
                params = {'org': organization}
                
                try:
                    response = requests.post(url, params=params, data=flux_query, headers=headers, timeout=30)
                    
                    if response.status_code == 200:
                        result = response.text.strip()
                        if result and not result.startswith('#'):
                            # Parse CSV-like output and format it better for error messages
                            lines = result.split('\n')
                            data_lines = [line for line in lines if line.strip() and not line.startswith('#')]
                            
                            if len(data_lines) > 1:
                                # Parse header
                                header_line = data_lines[0]
                                columns = header_line.split(',')
                                
                                # Find relevant column indices
                                try:
                                    backup_host_idx = columns.index('host') if 'host' in columns else -1
                                    field_idx = columns.index('_field') if '_field' in columns else -1
                                    value_idx = columns.index('_value') if '_value' in columns else -1
                                    time_idx = columns.index('_time') if '_time' in columns else -1
                                    path_idx = columns.index('path') if 'path' in columns else -1
                                except ValueError:
                                    # Fallback to basic display
                                    for i, line in enumerate(data_lines[:11]):
                                        print(f"  {line}")
                                        if i == 10 and len(data_lines) > 11:
                                            print(f"  ... and {len(data_lines) - 11} more")
                                    continue
                                
                                # Format error messages nicely
                                if query_name == "Error Messages":
                                    print("  Host-specific errors and warnings:")
                                    for line in data_lines[1:]:
                                        parts = line.split(',')
                                        if len(parts) > max(backup_host_idx, field_idx, value_idx, time_idx):
                                            backup_host = parts[backup_host_idx] if backup_host_idx >= 0 else 'unknown'
                                            field = parts[field_idx] if '_field' in columns else 'unknown'
                                            value = parts[value_idx] if value_idx >= 0 else 'unknown'
                                            timestamp = parts[time_idx] if time_idx >= 0 else 'unknown'
                                            
                                            # Clean up timestamp
                                            if timestamp != 'unknown':
                                                try:
                                                    # Convert from ISO format to readable
                                                    from datetime import datetime
                                                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                                    timestamp = dt.strftime('%Y-%m-%d %H:%M:%S')
                                                except:
                                                    pass
                                            
                                            # Clean up value (remove quotes if present)
                                            value = value.strip('"').replace('\\"', '"')
                                            
                                            error_type = "ERROR" if field == "last_error" else "WARNING"
                                            print(f"    [{timestamp}] {backup_host}: {error_type}")
                                            print(f"      {value}")
                                
                                elif query_name == "Volume Status (Errors)":
                                    print("  Volumes with errors (non-zero exit codes):")
                                    for line in data_lines[1:]:
                                        parts = line.split(',')
                                        if len(parts) > max(backup_host_idx, field_idx, value_idx, path_idx):
                                            backup_host = parts[backup_host_idx] if backup_host_idx >= 0 else 'unknown'
                                            path = parts[path_idx] if path_idx >= 0 else 'unknown'
                                            field = parts[field_idx] if field_idx >= 0 else 'unknown'
                                            value = parts[value_idx] if value_idx >= 0 else 'unknown'
                                            
                                            if field == "exit_code" and value != "0":
                                                print(f"    {backup_host}:{path} - Exit code: {value}")
                                            elif field == "status_numeric" and value not in ["1.0", "1"]:
                                                status = "WARNING" if value == "0.5" else "FAILED"
                                                print(f"    {backup_host}:{path} - Status: {status} ({value})")
                                
                                else:
                                    # Standard display for other queries
                                    for i, line in enumerate(data_lines[:11]):
                                        print(f"  {line}")
                                        if i == 10 and len(data_lines) > 11:
                                            remaining = len(data_lines) - 11
                                            print(f"  ... and {remaining} more")
                                            break
                            else:
                                print("  No data found")
                        else:
                            print("  No data found")
                    else:
                        print(f"  Query failed: HTTP {response.status_code} - {response.text}")
                        
                except Exception as e:
                    print(f"  Query error: {e}")
        else:
            print(f"Querying InfluxDB 1.x data from {influx_host}:{influx_port} (last {hours} hours)")
            print(f"Database: {database}")
            
            # Enhanced InfluxDB 1.x queries that look for error messages
            queries = {
                "Backup Status": f'SELECT * FROM "backup_status" WHERE time > now() - {hours}h ORDER BY time DESC LIMIT 10',
                "Host Status (Basic)": f'SELECT host, status_numeric, error_count, volume_count FROM "backup_host_status" WHERE time > now() - {hours}h ORDER BY time DESC LIMIT 20',
                "Error Messages": f'SELECT host, last_error, last_warning FROM "backup_host_status" WHERE time > now() - {hours}h AND (last_error != \'\' OR last_warning != \'\') ORDER BY time DESC LIMIT 20',
                "Volume Errors": f'SELECT host, path, exit_code, status_numeric FROM "backup_volume_status" WHERE time > now() - {hours}h AND (exit_code != 0 OR status_numeric != 1.0) ORDER BY time DESC LIMIT 20',
                "Host Sizes": f'SELECT host, size_bytes, file_count FROM "backup_host_size" WHERE time > now() - {hours}h ORDER BY time DESC LIMIT 10'
            }
            
            for measurement, influx_query in queries.items():
                print(f"\n=== {measurement} ===")
                
                url = f"{protocol}://{influx_host}:{influx_port}/query"
                params = {
                    'db': database,
                    'q': influx_query
                }
                
                if username and password:
                    params['u'] = username
                    params['p'] = password
                
                try:
                    response = requests.get(url, params=params, timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if 'results' in data and data['results']:
                            result = data['results'][0]
                            if 'series' in result and result['series']:
                                series = result['series'][0]
                                columns = series.get('columns', [])
                                values = series.get('values', [])
                                
                                if values:
                                    # Format error messages nicely for InfluxDB 1.x too
                                    if measurement == "Error Messages":
                                        print("  Recent errors and warnings:")
                                        for row in values:
                                            row_dict = dict(zip(columns, row))
                                            backup_host = row_dict.get('host', 'unknown')
                                            last_error = row_dict.get('last_error', '')
                                            last_warning = row_dict.get('last_warning', '')
                                            timestamp = row_dict.get('time', '')
                                            
                                            if last_error:
                                                print(f"    [{timestamp}] {backup_host}: ERROR")
                                                print(f"      {last_error}")
                                            if last_warning:
                                                print(f"    [{timestamp}] {backup_host}: WARNING") 
                                                print(f"      {last_warning}")
                                    else:
                                        # Standard display
                                        print(f"  Columns: {', '.join(columns)}")
                                        for i, row in enumerate(values[:5]):
                                            print(f"  Row {i+1}: {row}")
                                        if len(values) > 5:
                                            print(f"  ... and {len(values) - 5} more")
                                else:
                                    print("  No data found")
                            else:
                                print("   No data found")
                        else:
                            print("  No data found")
                    else:
                        print(f"  Query failed: HTTP {response.status_code} - {response.text}")
                        
                except Exception as e:
                    print(f"  Query error: {e}")
        
        return True

    def query_backup_status(self, runs: int = 1, scope: str = "run", hours: int = 168):
        """Query backup status from InfluxDB with various scopes and run counts"""
        try:
            import requests
        except ImportError:
            print("Error: requests library not found. Install with: pip install requests")
            return False
        
        influx_cfg = self.influxdb_config.get('influxdb', {})
        influx_host = influx_cfg.get('host', 'localhost')
        influx_port = influx_cfg.get('port', 8086)
        ssl = influx_cfg.get('ssl', False)
        
        # InfluxDB 2.x support
        token = str(influx_cfg.get('token', '')).strip()
        organization = str(influx_cfg.get('organization', '')).strip()
        bucket = str(influx_cfg.get('bucket', '')).strip()
        
        if not organization and token:
            organization = 'home'
        
        # InfluxDB 1.x support
        database = influx_cfg.get('database', 'backup_metrics')
        username = influx_cfg.get('username', '')
        password = influx_cfg.get('password', '')
        
        is_influxdb_2x = bool(token and organization)
        protocol = "https" if ssl else "http"
        
        print(f"Querying backup status from InfluxDB (last {runs} run(s), scope: {scope})")
        print(f"Target: {influx_host}:{influx_port}")
        
        if scope == "run":
            return self._query_run_status(is_influxdb_2x, protocol, influx_host, influx_port, 
                                        token, organization, bucket, database, username, password, runs, hours)
        elif scope == "host":
                       return self._query_host_status(is_influxdb_2x, protocol, influx_host, influx_port,
                                         token, organization, bucket, database, username, password, runs, hours)
        elif scope == "volume":
            return self._query_volume_status(is_influxdb_2x, protocol, influx_host, influx_port,
                                           token, organization, bucket, database, username, password, runs, hours)
        elif scope == "all":
            print("\n" + "="*80)
            print("COMPREHENSIVE BACKUP STATUS REPORT")
            print("="*80)
            
            print("\n📊 RUN STATUS:")
            print("-" * 40)
            self._query_run_status(is_influxdb_2x, protocol, influx_host, influx_port,
                                 token, organization, bucket, database, username, password, runs, hours)
            
            print("\n🖥️  HOST STATUS:")
            print("-" * 40)
            self._query_host_status(is_influxdb_2x, protocol, influx_host, influx_port,
                                  token, organization, bucket, database, username, password, runs, hours)
            
            print("\n💾 VOLUME STATUS:")
            print("-" * 40)
            self._query_volume_status(is_influxdb_2x, protocol, influx_host, influx_port,
                                    token, organization, bucket, database, username, password, runs, hours)
            return True
        else:
            print(f"Error: Unknown scope '{scope}'. Valid options: run, host, volume, all")
            return False

    def _query_run_status(self, is_influxdb_2x, protocol, influx_host, influx_port,
                         token, organization, bucket, database, username, password, runs, hours):
        """Query overall backup run status"""
        
        if is_influxdb_2x:
            # Get the last N backup runs using backup_status measurement
            flux_query = f'''
                from(bucket: "{bucket}")
                |> range(start: -{hours}h)
                |> filter(fn: (r) => r._measurement == "backup_status")
                |> filter(fn: (r) => r._field == "status_numeric" or r._field == "duration_seconds" or r._field == "error_count" or r._field == "total_size_bytes")
                |> pivot(rowKey: ["_time"], columnKey: ["_field"], valueColumn: "_value")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {runs})
            '''
            
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'application/vnd.flux'
            }
            params = {'org': organization}
            url = f"{protocol}://{influx_host}:{port}/api/v2/query"
            
        else:
            # InfluxDB 1.x query
            influx_query = f'''
                SELECT status_numeric, duration_seconds, error_count, total_size_bytes
                FROM "backup_status"
                WHERE time > now() - {hours}h
                ORDER BY time DESC
                LIMIT {runs}
            '''
            
            params = {
                'db': database,
                'q': influx_query
            }
            if username and password:
                params['u'] = username
                params['p'] = password
            
            url = f"{protocol}://{influx_host}:{port}/query"
        
        try:
            if is_influxdb_2x:
                response = requests.post(url, params=params, data=flux_query, headers=headers, timeout=30)
            else:
                response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                if is_influxdb_2x:
                    self._parse_flux_run_status(response.text, runs)
                else:
                    data = response.json()
                    self._parse_influx1_run_status(data, runs)
                return True
            else:
                print(f"Query failed: HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"Query error: {e}")
            return False

    def _query_host_status(self, is_influxdb_2x, protocol, influx_host, influx_port,
                          token, organization, bucket, database, username, password, runs, hours):
        """Query host status across multiple runs"""
        
        if is_influxdb_2x:
            # Get host session data for the last N runs
            flux_query = f'''
                from(bucket: "{bucket}")
                |> range(start: -{hours}h)
                |> filter(fn: (r) => r._measurement == "backup_host_session")
                |> filter(fn: (r) => r._field == "status_numeric" or r._field == "duration_seconds" or r._field == "error_count" or r._field == "warning_count")
                |> pivot(rowKey: ["_time", "host"], columnKey: ["_field"], valueColumn: "_value")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {runs * 10})
            '''
            
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'application/vnd.flux'
            }
            params = {'org': organization}
            url = f"{protocol}://{influx_host}:{port}/api/v2/query"
            
        else:
            # InfluxDB 1.x query
            influx_query = f'''
                SELECT host, status_numeric, duration_seconds, error_count, warning_count
                FROM "backup_host_session"
                WHERE time > now() - {hours}h
                ORDER BY time DESC
                LIMIT {runs * 10}
            '''
            
            params = {
                'db': database,
                'q': influx_query
            }
            if username and password:
                params['u'] = username
                params['p'] = password
            
            url = f"{protocol}://{influx_host}:{port}/query"
        
        try:
            if is_influxdb_2x:
                response = requests.post(url, params=params, data=flux_query, headers=headers, timeout=30)
            else:
                response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                if is_influxdb_2x:
                    self._parse_flux_host_status(response.text, runs)
                else:
                    data = response.json()
                    self._parse_influx1_host_status(data, runs)
                return True
            else:
                print(f"Query failed: HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"Query error: {e}")
            return False

    def _query_volume_status(self, is_influxdb_2x, protocol, influx_host, influx_port,
                            token, organization, bucket, database, username, password, runs, hours):
        """Query volume status across multiple runs"""
        
        if is_influxdb_2x:
            # Get volume session data for the last N runs
            flux_query = f'''
                from(bucket: "{bucket}")
                |> range(start: -{hours}h)
                |> filter(fn: (r) => r._measurement == "backup_volume_session")
                |> filter(fn: (r) => r._field == "status_numeric" or r._field == "exit_code")
                |> pivot(rowKey: ["_time", "host", "path"], columnKey: ["_field"], valueColumn: "_value")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {runs * 50})
            '''
            
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'application/vnd.flux'
            }
            params = {'org': organization}
            url = f"{protocol}://{influx_host}:{port}/api/v2/query"
            
        else:
            # InfluxDB 1.x query
            influx_query = f'''
                SELECT host, path, status_numeric, exit_code
                FROM "backup_volume_session"
                WHERE time > now() - {hours}h
                ORDER BY time DESC
                LIMIT {runs * 50}
            '''
            
            params = {
                'db': database,
                'q': influx_query
            }
            if username and password:
                params['u'] = username
                params['p'] = password
            
            url = f"{protocol}://{influx_host}:{port}/query"
        
        try:
            if is_influxdb_2x:
                response = requests.post(url, params=params, data=flux_query, headers=headers, timeout=30)
            else:
                response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                if is_influxdb_2x:
                    self._parse_flux_volume_status(response.text, runs)
                else:
                    data = response.json()
                    self._parse_influx1_volume_status(data, runs)
                return True
            else:
                print(f"Query failed: HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"Query error: {e}")
            return False

    def _parse_flux_run_status(self, flux_result, runs):
        """Parse Flux query results for run status"""
        lines = flux_result.strip().split('\n')
        data_lines = [line for line in lines if line.strip() and not line.startswith('#')]
        
        if len(data_lines) < 2:
            print("No backup run data found")
            return
        
        # Parse header and data
        header = data_lines[0].split(',')
        
        print(f"📊 Last {min(runs, len(data_lines)-1)} Backup Run(s):")
        print("-" * 80)
        print(f"{'Run':<4} {'Time':<20} {'Status':<10} {'Duration':<10} {'Errors':<8} {'Size (GB)':<12}")
        print("-" * 80)
        
        for i, line in enumerate(data_lines[1:runs+1]):
            parts = line.split(',')
            if len(parts) >= len(header):
                row_data = dict(zip(header, parts))
                
                time_str = row_data.get('_time', '').replace('T', ' ').replace('Z', '')[:19]
                status_num = float(row_data.get('status_numeric', 0))
                duration = float(row_data.get('duration_seconds', 0))
                errors = int(float(row_data.get('error_count', 0)))
                size_bytes = int(float(row_data.get('total_size_bytes', 0)))
                
                status_text = "✓ SUCCESS" if status_num == 1.0 else "⚠ WARNING" if status_num == 0.5 else "❌ FAILED"
                duration_text = f"{duration:.0f}s"
                size_gb = size_bytes / (1024**3) if size_bytes > 0 else 0
                size_text = f"{size_gb:.2f}" if size_gb > 0 else "N/A"
                
                print(f"{i+1:<4} {time_str:<20} {status_text:<10} {duration_text:<10} {errors:<8} {size_text:<12}")

    def _query_run_status(self, is_influxdb_2x, protocol, influx_host, influx_port,
                         token, organization, bucket, database, username, password, runs, hours):
        """Query overall backup run status"""
        
        if is_influxdb_2x:
            # Get the last N backup runs using backup_status measurement
            flux_query = f'''
                from(bucket: "{bucket}")
                |> range(start: -{hours}h)
                |> filter(fn: (r) => r._measurement == "backup_status")
                |> filter(fn: (r) => r._field == "status_numeric" or r._field == "duration_seconds" or r._field == "error_count" or r._field == "total_size_bytes")
                |> pivot(rowKey: ["_time"], columnKey: ["_field"], valueColumn: "_value")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {runs})
            '''
            
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'application/vnd.flux'
            }
            params = {'org': organization}
            url = f"{protocol}://{influx_host}:{port}/api/v2/query"
            
        else:
            # InfluxDB 1.x query
            influx_query = f'''
                SELECT status_numeric, duration_seconds, error_count, total_size_bytes
                FROM "backup_status"
                WHERE time > now() - {hours}h
                ORDER BY time DESC
                LIMIT {runs}
            '''
            
            params = {
                'db': database,
                'q': influx_query
            }
            if username and password:
                params['u'] = username
                params['p'] = password
            
            url = f"{protocol}://{influx_host}:{port}/query"
        
        try:
            if is_influxdb_2x:
                response = requests.post(url, params=params, data=flux_query, headers=headers, timeout=30)
            else:
                response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                if is_influxdb_2x:
                    self._parse_flux_run_status(response.text, runs)
                else:
                    data = response.json()
                    self._parse_influx1_run_status(data, runs)
                return True
            else:
                print(f"Query failed: HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"Query error: {e}")
            return False

    def _query_host_status(self, is_influxdb_2x, protocol, influx_host, influx_port,
                          token, organization, bucket, database, username, password, runs, hours):
        """Query host status across multiple runs"""
        
        if is_influxdb_2x:
            # Get host session data for the last N runs
            flux_query = f'''
                from(bucket: "{bucket}")
                |> range(start: -{hours}h)
                |> filter(fn: (r) => r._measurement == "backup_host_session")
                |> filter(fn: (r) => r._field == "status_numeric" or r._field == "duration_seconds" or r._field == "error_count" or r._field == "warning_count")
                |> pivot(rowKey: ["_time", "host"], columnKey: ["_field"], valueColumn: "_value")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {runs * 10})
            '''
            
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'application/vnd.flux'
            }
            params = {'org': organization}
            url = f"{protocol}://{influx_host}:{port}/api/v2/query"
            
        else:
            # InfluxDB 1.x query
            influx_query = f'''
                SELECT host, status_numeric, duration_seconds, error_count, warning_count
                FROM "backup_host_session"
                WHERE time > now() - {hours}h
                ORDER BY time DESC
                LIMIT {runs * 10}
            '''
            
            params = {
                'db': database,
                'q': influx_query
            }
            if username and password:
                params['u'] = username
                params['p'] = password
            
            url = f"{protocol}://{influx_host}:{port}/query"
        
        try:
            if is_influxdb_2x:
                response = requests.post(url, params=params, data=flux_query, headers=headers, timeout=30)
            else:
                response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                if is_influxdb_2x:
                    self._parse_flux_host_status(response.text, runs)
                else:
                    data = response.json()
                    self._parse_influx1_host_status(data, runs)
                return True
            else:
                print(f"Query failed: HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"Query error: {e}")
            return False

    def _query_volume_status(self, is_influxdb_2x, protocol, influx_host, influx_port,
                            token, organization, bucket, database, username, password, runs, hours):
        """Query volume status across multiple runs"""
        
        if is_influxdb_2x:
            # Get volume session data for the last N runs
            flux_query = f'''
                from(bucket: "{bucket}")
                |> range(start: -{hours}h)
                |> filter(fn: (r) => r._measurement == "backup_volume_session")
                |> filter(fn: (r) => r._field == "status_numeric" or r._field == "exit_code")
                |> pivot(rowKey: ["_time", "host", "path"], columnKey: ["_field"], valueColumn: "_value")
                |> sort(columns: ["_time"], desc: true)
                |> limit(n: {runs * 50})
            '''
            
            headers = {
                'Authorization': f'Token {token}',
                'Content-Type': 'application/vnd.flux'
            }
            params = {'org': organization}
            url = f"{protocol}://{influx_host}:{port}/api/v2/query"
            
        else:
            # InfluxDB 1.x query
            influx_query = f'''
                SELECT host, path, status_numeric, exit_code
                FROM "backup_volume_session"
                WHERE time > now() - {hours}h
                ORDER BY time DESC
                LIMIT {runs * 50}
            '''
            
            params = {
                'db': database,
                'q': influx_query
            }
            if username and password:
                params['u'] = username
                params['p'] = password
            
            url = f"{protocol}://{influx_host}:{port}/query"
        
        try:
            if is_influxdb_2x:
                response = requests.post(url, params=params, data=flux_query, headers=headers, timeout=30)
            else:
                response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                if is_influxdb_2x:
                    self._parse_flux_volume_status(response.text, runs)
                else:
                    data = response.json()
                    self._parse_influx1_volume_status(data, runs)
                return True
            else:
                print(f"Query failed: HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"Query error: {e}")
            return False

    def _parse_flux_run_status(self, flux_result, runs):
        """Parse Flux query results for run status"""
        lines = flux_result.strip().split('\n')
        data_lines = [line for line in lines if line.strip() and not line.startswith('#')]
        
        if len(data_lines) < 2:
            print("No backup run data found")
            return
        
        # Parse header and data
        header = data_lines[0].split(',')
        
        print(f"📊 Last {min(runs, len(data_lines)-1)} Backup Run(s):")
        print("-" * 80)
        print(f"{'Run':<4} {'Time':<20} {'Status':<10} {'Duration':<10} {'Errors':<8} {'Size (GB)':<12}")
        print("-" * 80)
        
        for i, line in enumerate(data_lines[1:runs+1]):
            parts = line.split(',')
            if len(parts) >= len(header):
                row_data = dict(zip(header, parts))
                
                time_str = row_data.get('_time', '').replace('T', ' ').replace('Z', '')[:19]
                status_num = float(row_data.get('status_numeric', 0))
                duration = float(row_data.get('duration_seconds', 0))
                errors = int(float(row_data.get('error_count', 0)))
                size_bytes = int(float(row_data.get('total_size_bytes', 0)))
                
                status_text = "✓ SUCCESS" if status_num == 1.0 else "⚠ WARNING" if status_num == 0.5 else "❌ FAILED"
                duration_text = f"{duration:.0f}s"
                size_gb = size_bytes / (1024**3) if size_bytes > 0 else 0
                size_text = f"{size_gb:.2f}" if size_gb > 0 else "N/A"
                
                print(f"{i+1:<4} {time_str:<20} {status_text:<10} {duration_text:<10} {errors:<8} {size_text:<12}")

    def _parse_flux_host_status(self, flux_result, runs):
        """Parse Flux query results for host status"""
        lines = flux_result.strip().split('\n')
        data_lines = [line for line in lines if line.strip() and not line.startswith('#')]
        
        if len(data_lines) < 2:
            print("No host backup data found")
            return
        
        # Parse data and organize by time/run and host
        header = data_lines[0].split(',')
        host_data = {}
        run_times = set()
        
        for line in data_lines[1:]:
            parts = line.split(',')
            if len(parts) >= len(header):
                row_data = dict(zip(header, parts))
                
                time_str = row_data.get('_time', '')
                host = row_data.get('host', 'unknown')
                status_num = float(row_data.get('status_numeric', 0))
                duration = float(row_data.get('duration_seconds', 0))
                errors = int(float(row_data.get('error_count', 0)))
                warnings = int(float(row_data.get('warning_count', 0)))
                
                run_times.add(time_str)
                
                if time_str not in host_data:
                    host_data[time_str] = {}
                
                host_data[time_str][host] = {
                    'status': status_num,
                    'duration': duration,
                    'errors': errors,
                    'warnings': warnings
                }
        
        # Sort run times (most recent first)
        sorted_times = sorted(run_times, reverse=True)[:runs]
        all_hosts = set()
        
        for time_str in sorted_times:
            if time_str in host_data:
                all_hosts.update(host_data[time_str].keys())
        
        all_hosts = sorted(all_hosts)
        
        print(f"🖥️  Host Status Matrix (Last {len(sorted_times)} Run(s)):")
        print("-" * 80)
        
        # Print header
        header_line = f"{'Host':<15}"
        for i, time_str in enumerate(sorted_times):
            run_label = f"Run {i+1}"
            header_line += f"{run_label:<12}"
        print(header_line)
        print("-" * 80)
        
        # Print host status for each run
        for host in all_hosts:
            row = f"{host:<15}"
            for time_str in sorted_times:
                if time_str in host_data and host in host_data[time_str]:
                    data = host_data[time_str][host]
                    status_num = data['status']
                    errors = data['errors']
                    warnings = data['warnings']
                    
                    if status_num == 1.0:
                        status_text = "✓ OK" if errors == 0 and warnings == 0 else f"✓ OK({warnings}w)"
                    elif status_num == 0.5:
                        status_text = f"⚠ WARN({warnings})"
                    else:
                        status_text = f"❌ FAIL({errors})"
                    
                    row += f"{status_text:<12}"
                else:
                    row += f"{'—':<12}"
            print(row)

    def _parse_flux_volume_status(self, flux_result, runs):
        """Parse Flux query results for volume status"""
        lines = flux_result.strip().split('\n')
        data_lines = [line for line in lines if line.strip() and not line.startswith('#')]
        
        if len(data_lines) < 2:
            print("No volume backup data found")
            return
        
        # Parse data and organize by host:path
        header = data_lines[0].split(',')
        volume_data = {}
        run_times = set()
        
        for line in data_lines[1:]:
            parts = line.split(',')
            if len(parts) >= len(header):
                row_data = dict(zip(header, parts))
                
                time_str = row_data.get('_time', '')
                host = row_data.get('host', 'unknown')
                path = row_data.get('path', 'unknown')
                status_num = float(row_data.get('status_numeric', 0))
                exit_code = int(row_data.get('exit_code', 0))
                
                volume_key = f"{host}:{path}"
                run_times.add(time_str)
                
                if time_str not in volume_data:
                    volume_data[time_str] = {}
                
                volume_data[time_str][volume_key] = {
                    'status': status_num,
                    'exit_code': exit_code
                }
        
        # Display logic same as Flux version
        sorted_times = sorted(run_times, reverse=True)[:runs]
        all_volumes = set()
        
        for time_str in sorted_times:
            if time_str in volume_data:
                all_volumes.update(volume_data[time_str].keys())
        
        all_volumes = sorted(all_volumes)
        
        print(f"💾 Volume Status Matrix (Last {len(sorted_times)} Run(s)):")
        print("-" * 100)
        
        # Print header
        header_line = f"{'Volume':<35}"
        for i, time_str in enumerate(sorted_times):
            run_label = f"Run {i+1}"
            header_line += f"{run_label:<12}"
        print(header_line)
        print("-" * 100)
        
        # Print volume status for each run
        for volume in all_volumes:
            # Truncate long volume names
            volume_display = volume[:32] + "..." if len(volume) > 35 else volume
            row = f"{volume_display:<35}"
            
            for time_str in sorted_times:
                if time_str in volume_data and volume in volume_data[time_str]:
                    data = volume_data[time_str][volume]
                    status_num = data['status']
                    exit_code = data['exit_code']
                    
                    if status_num == 1.0 and exit_code == 0:
                        status_text = "✓ OK"
                    elif status_num == 0.5:
                        status_text = f"⚠ W({exit_code})"
                    else:
                        status_text = f"❌ F({exit_code})"
                    
                    row += f"{status_text:<12}"
                else:
                    row += f"{'—':<12}"
            print(row)

    def _detect_backup_frequency(self) -> int:
        """Detect backup frequency from systemd timer configuration and return search hours"""
        if self.verbose:
            print("[DEBUG] Auto-detecting backup frequency...")
        
        # Try to get service name from InfluxDB config
        service_name = self.influxdb_config.get('systemd', {}).get('service_name', 'backup.service')
        timer_name = service_name.replace('.service', '.timer')
        
        # Common timer locations to check
        timer_paths = [
            f"/etc/systemd/system/{timer_name}",
            f"/etc/systemd/user/{timer_name}",
            f"/home/{os.getenv('USER', 'pjakobs')}/.config/systemd/user/{timer_name}",
            f"/usr/lib/systemd/system/{timer_name}",
            f"/usr/lib/systemd/user/{timer_name}"
        ]
        
        for timer_path in timer_paths:
            try:
                if Path(timer_path).exists():
                    if self.verbose:
                        print(f"[DEBUG] Found timer file: {timer_path}")
                    
                    with open(timer_path, 'r') as f:
                        content = f.read()
                    
                    # Look for OnCalendar directive
                    for line in content.split('\n'):
                        line = line.strip()
                        if line.startswith('OnCalendar='):
                            calendar_spec = line.split('=', 1)[1].strip()
                            if self.verbose:
                                print(f"[DEBUG] Found OnCalendar: {calendar_spec}")
                            
                            # Parse common patterns
                            if '/2:00' in calendar_spec:  # Every 2 hours
                                if self.verbose:
                                    print("[DEBUG] Detected 2-hour backup interval")
                                return 6  # Search last 6 hours for 2-hourly backups
                            elif 'hourly' in calendar_spec.lower() or '1:00' in calendar_spec:
                                if self.verbose:
                                    print("[DEBUG] Detected hourly backup interval")
                                return 3  # Search last 3 hours for hourly backups
                            elif 'daily' in calendar_spec.lower() or '*-*-*' in calendar_spec:
                                if self.verbose:
                                    print("[DEBUG] Detected daily backup interval")
                                return 48  # Search last 48 hours for daily backups
                            elif 'weekly' in calendar_spec.lower():
                                if self.verbose:
                                    print("[DEBUG] Detected weekly backup interval")
                                return 168  # Search last 7 days for weekly backups
                            else:
                                if self.verbose:
                                    print(f"[DEBUG] Unknown calendar pattern: {calendar_spec}")
                                
            except Exception as e:
                if self.verbose:
                    print(f"[DEBUG] Could not read {timer_path}: {e}")
                continue
        
        # Fallback: Try to detect from actual backup intervals in logs
        if self.verbose:
            print("[DEBUG] Timer file not found, analyzing actual backup intervals...")
        
        try:
            # Get last few backup sessions and calculate average interval
            logs = self.parse_systemd_logs(hours=168)  # Check last week
            sessions = self.extract_backup_sessions(logs)
            
            if len(sessions) >= 2:
                sessions.sort(key=lambda x: x['start_timestamp'])
                intervals = []
                
                for i in range(1, min(len(sessions), 6)):  # Check up to 5 intervals
                    interval_hours = (sessions[i]['start_timestamp'] - sessions[i-1]['start_timestamp']) / 3600
                    intervals.append(interval_hours)
                
                if intervals:
                    avg_interval = sum(intervals) / len(intervals)
                    if self.verbose:
                        print(f"[DEBUG] Average backup interval: {avg_interval:.1f} hours")
                    
                    # Choose search window based on average interval
                    if avg_interval <= 1.5:
                        return 3  # Hourly
                    elif avg_interval <= 3:
                        return 6  # 2-hourly  
                    elif avg_interval <= 6:
                        return 12  # 4-6 hourly
                    elif avg_interval <= 26:
                        return 48  # Daily
                    else:
                        return 168  # Weekly or less frequent
                        
        except Exception as e:
            if self.verbose:
                print(f"[DEBUG] Could not analyze backup intervals: {e}")
        
        # Final fallback - assume 2-hourly backups (common default)
        if self.verbose:
            print("[DEBUG] Using default assumption: 2-hourly backups")
        return 6

    def print_last_runs(self, run_offset: int = 0, number_of_runs: int = 1, 
                       filter_host: str = None, hours: int = 24):
        """Print backup run logs with specified parameters"""
        if self.verbose:
            print(f"[DEBUG] Printing last runs: offset={run_offset}, count={number_of_runs}, host={filter_host}, hours={hours}")
        
        # Get logs from systemd
        logs = self.parse_systemd_logs(hours=hours)
        
        # Extract sessions using the precise method for better run tracking
        host_sessions = self.extract_host_sessions_precise(logs)
        
        if not host_sessions:
            print("No backup runs found in the specified time range.")
            return
        
        # Group sessions by run_id or time proximity for run-based display
        runs = {}
        for session in host_sessions:
            # Try to get run_id first, fall back to time-based grouping
            run_key = session.get('run_id')
            if not run_key:
                # Group by 10-minute windows for sessions without run_id
                time_window = int(session['start_timestamp'] // 600) * 600
                run_key = f"time_{time_window}"
            
            if run_key not in runs:
                runs[run_key] = {
                    'start_time': session['start_time'],
                    'start_timestamp': session['start_timestamp'],
                    'sessions': [],
                    'run_id': session.get('run_id', run_key)
                }
            
            runs[run_key]['sessions'].append(session)
            # Update run start time to earliest session
            if session['start_timestamp'] < runs[run_key]['start_timestamp']:
                runs[run_key]['start_time'] = session['start_time']
                runs[run_key]['start_timestamp'] = session['start_timestamp']
        
        # Sort runs by start time (newest first)
        sorted_runs = sorted(runs.values(), key=lambda x: x['start_timestamp'], reverse=True)
        
        # Apply offset and count
        if run_offset >= len(sorted_runs):
            print(f"Offset {run_offset} is beyond available runs ({len(sorted_runs)} found)")
            return
        
        start_idx = run_offset
        end_idx = min(start_idx + number_of_runs, len(sorted_runs))
        selected_runs = sorted_runs[start_idx:end_idx]
        
        # Display runs
        for run_idx, run_data in enumerate(selected_runs):
            run_num = run_offset + run_idx + 1
            print(f"\n{'='*80}")
            print(f"BACKUP RUN #{run_num} (ID: {run_data['run_id']})")
            print(f"Started: {run_data['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'='*80}")
            
            # Filter sessions by host if specified
            sessions_to_show = run_data['sessions']
            if filter_host:
                sessions_to_show = [s for s in sessions_to_show if s['hostname'] == filter_host]
                if not sessions_to_show:
                    print(f"No sessions found for host '{filter_host}' in this run")
                    continue
            
            # Sort sessions by start time
            sessions_to_show.sort(key=lambda x: x['start_timestamp'])
            
            # Display session summary
            total_sessions = len(sessions_to_show)
            success_count = len([s for s in sessions_to_show if s['status'] == 'success'])
            warning_count = len([s for s in sessions_to_show if s['status'] == 'warning'])
            failed_count = len([s for s in sessions_to_show if s['status'] == 'failed'])
            running_count = len([s for s in sessions_to_show if s['status'] == 'running'])
            
            print(f"\nSUMMARY: {total_sessions} host(s) - "
                  f"✓ {success_count} success, ⚠ {warning_count} warnings, "
                  f"❌ {failed_count} failed, 🔄 {running_count} running")
            
            # Display individual host sessions
            for session in sessions_to_show:
                hostname = session['hostname']
                status = session['status']
                duration = session.get('duration', 0)
                
                # Status symbol
                status_symbol = {
                    'success': '✓',
                    'warning': '⚠', 
                    'failed': '❌',
                    'running': '🔄',
                    'interrupted': '⚡'
                }.get(status, '❓')
                
                # Time info
                start_time = session['start_time'].strftime('%H:%M:%S')
                end_time = session.get('end_time')
                end_time_str = end_time.strftime('%H:%M:%S') if end_time else "ongoing"
                
                print(f"\n{status_symbol} {hostname.upper()}")
                print(f"  Time: {start_time} → {end_time_str} ({duration:.1f}s)")
                
                # Path details
                paths = session.get('paths', [])
                if paths:
                    print(f"  Paths: {len(paths)} volume(s)")
                    for path_info in paths:
                        path = path_info['path']
                        path_status = path_info['status']
                        path_symbol = '✓' if path_status == 'success' else '⚠' if path_status == 'warning' else '❌'
                        exit_code = path_info.get('exit_code', 0)
                        exit_info = f" (exit: {exit_code})" if exit_code != 0 else ""
                        print(f"    {path_symbol} {path}{exit_info}")
                
                # Error and warning details
                errors = session.get('errors', [])
                warnings = session.get('warnings', [])
                
                if errors:
                    print(f"  Errors: {len(errors)}")
                    for error in errors[-3:]:  # Show last 3 errors
                        error_msg = error['message'][:100] + "..." if len(error['message']) > 100 else error['message']
                        print(f"    ❌ {error_msg}")
                    if len(errors) > 3:
                        print(f"    ... and {len(errors) - 3} more errors")
                
                if warnings:
                    print(f"  Warnings: {len(warnings)}")
                    for warning in warnings[-2:]:  # Show last 2 warnings
                        warning_msg = warning['message'][:100] + "..." if len(warning['message']) > 100 else warning['message']
                        print(f"    ⚠ {warning_msg}")
                    if len(warnings) > 2:
                        print(f"    ... and {len(warnings) - 2} more warnings")
        
        # Footer summary
        print(f"\n{'='*80}")
        print(f"Showing {len(selected_runs)} run(s) starting from run #{run_offset + 1}")
        if filter_host:
            print(f"Filtered for host: {filter_host}")
        print(f"Search range: last {hours} hours")
        print(f"Total runs available: {len(sorted_runs)}")


def main():
    # Parse command line arguments for verbose mode
    verbose = False
    if "--verbose" in sys.argv or "-v" in sys.argv:
        verbose = True
        # Remove verbose flags from sys.argv so other argument parsing works
        sys.argv = [arg for arg in sys.argv if arg not in ["--verbose", "-v"]]
    
    collector = BackupMetricsCollector(verbose=verbose)
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--status":
            # Parse --status options
            runs = 1  # Default to 1 run
            scope = "run"  # Default scope
            hours = 168  # Default to 1 week
            
            # Parse additional arguments
            i = 2
            while i < len(sys.argv):
                arg = sys.argv[i]
                
                if arg.startswith("--runs="):
                    try:
                        runs = int(arg.split("=", 1)[1])
                        if runs < 1:
                            print("Error: --runs must be at least 1")
                            sys.exit(1)
                    except ValueError:
                        print(f"Error: Invalid number in '{arg}'")
                        sys.exit(1)
                elif arg.startswith("--scope="):
                    scope = arg.split("=", 1)[1]
                    if scope not in ["run", "host", "volume", "all"]:
                        print(f"Error: Invalid scope '{scope}'. Valid options: run, host, volume, all")
                        sys.exit(1)
                elif arg.startswith("--hours="):
                    try:
                        hours = int(arg.split("=", 1)[1])
                        if hours < 1:
                            print("Error: --hours must be at least 1")
                            sys.exit(1)
                    except ValueError:
                        print(f"Error: Invalid number in '{arg}'")
                        sys.exit(1)
                else:
                    print(f"Error: Unknown argument '{arg}' for --status")
                    print("Usage: --status [--runs=N] [--scope=run|host|volume|all] [--hours=N]")
                    sys.exit(1)
                
                i += 1
            
            # Execute status query
            success = collector.query_backup_status(runs, scope, hours)
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == "--last-run":
            # Parse --last-run options
            run_offset = 0  # Default: start from most recent run
            number_of_runs = 1  # Default: show 1 run
            filter_host = None  # Default: show all hosts
            hours = None  # Default: auto-detect
            
            # Parse additional arguments
            i = 2
            while i < len(sys.argv):
                arg = sys.argv[i]
                
                if arg.startswith("-") and arg[1:].isdigit():
                    # Handle -N syntax (run offset)
                    try:
                        run_offset = int(arg[1:])
                    except ValueError:
                        print(f"Error: Invalid run offset '{arg}'")
                        sys.exit(1)
                elif arg.startswith("--count="):
                    try:
                        number_of_runs = int(arg.split("=", 1)[1])
                        if number_of_runs < 1:
                            print("Error: --count must be at least 1")
                            sys.exit(1)
                    except ValueError:
                        print(f"Error: Invalid number in '{arg}'")
                        sys.exit(1)
                elif arg.startswith("--host="):
                    filter_host = arg.split("=", 1)[1]
                elif arg.startswith("--hours="):
                    try:
                        hours = int(arg.split("=", 1)[1])
                        if hours < 1:
                            print("Error: --hours must be at least 1")
                            sys.exit(1)
                    except ValueError:
                        print(f"Error: Invalid number in '{arg}'")
                        sys.exit(1)
                else:
                    print(f"Error: Unknown argument '{arg}' for --last-run")
                    print("Usage: --last-run [-N] [--count N] [--host HOST] [--hours N]")
                    sys.exit(1)
                
                i += 1
            
            # Auto-detect hours if not specified
            if hours is None:
                hours = collector._detect_backup_frequency()
                if verbose:
                    print(f"Auto-detected backup frequency: searching last {hours} hours")
            
            # Validate arguments
            if run_offset < 0:
                print("Error: Run offset cannot be negative")
                sys.exit(1)
            
            if number_of_runs < 1:
                print("Error: Number of runs must be at least 1")
                sys.exit(1)
            
            # Print the requested backup runs
            collector.print_last_runs(run_offset, number_of_runs, filter_host, hours)
            sys.exit(0)
            
        elif sys.argv[1] == "--send-influxdb":
            # Start background scan but don't wait for it
            collector.start_background_directory_scan()
            
            # Send immediate metrics with what we have
            success = collector.send_to_influxdb()
            
            print("Background directory scan continues in background")
            print("Individual host metrics will be updated to InfluxDB as scan progresses")
            
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == "--send-influxdb-wait":
            # Start background scan and wait for completion
            collector.start_background_directory_scan()
            
            print("Waiting for background directory scan to complete...")
            if collector.du_thread:
                collector.du_thread.join()
            
            success = collector.send_to_influxdb()
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == "--test-influxdb":
            # Test InfluxDB connection only
            success = collector.test_influxdb_connection()
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == "--query-data":
            # Query and display existing data from InfluxDB
            hours = 24
            if len(sys.argv) > 2:
                try:
                    hours = int(sys.argv[2])
                except ValueError:
                    print("Invalid hours parameter, using default 24 hours")
            success = collector.query_influxdb_data(hours)
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == "--delete-data":
            # Delete all backup data from InfluxDB - for development use only
            success = collector.delete_all_backup_data()
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == "--json":
            collector.start_background_directory_scan()
            metrics = collector.generate_metrics_fast()
            print(json.dumps(metrics, indent=2, default=lambda x: x.isoformat() if isinstance(x, datetime) else str(x)))
            
        elif sys.argv[1] == "--json-wait":
            # Start background scan and wait for completion, then output JSON
            collector.start_background_directory_scan()
            
            print("Waiting for background directory scan to complete...")
            if collector.du_thread:
                collector.du_thread.join()
            
            metrics = collector.generate_metrics_fast()
            print(json.dumps(metrics, indent=2, default=lambda x: x.isoformat() if isinstance(x, datetime) else str(x)))
            
        elif sys.argv[1] == "--start-background-scan":
            # Just start background scan and exit
            collector.start_background_directory_scan()
            print("Background scan started. Use --send-influxdb to send current metrics.")
            return
            
        elif sys.argv[1] == "--reset-state":
            # Reset sent data state - for development/testing
            collector.reset_sent_data_state()
            sys.exit(0)
            
        elif sys.argv[1] == "--help":
            print("Usage: backup-metrics [OPTIONS]")
            print("")
            print("Options:")
            print("  --status [--runs=N] [--scope=run|host|volume|all] [--hours=N]")
            print("                        Query backup status from InfluxDB")
            print("                        --runs=N: Number of recent runs to analyze (default: 1)")
            print("                        --scope=run|host|volume|all: Status scope (default: run)")
            print("                        --hours=N: How far back to search (default: 168)")
            print("  --last-run [-N] [--count N] [--host HOST] [--hours N]")
            print("                        Show backup run logs")
            print("  --send-influxdb       Send NEW metrics to InfluxDB (deduplication enabled)")
            print("  --send-influxdb-wait  Send NEW metrics after directory scan completes")
            print("  --reset-state         Reset deduplication state (next run sends all data)")
            print("  --test-influxdb       Test InfluxDB connection")
            print("  --query-data [hours]  Query and display data from InfluxDB")
            print("  --delete-data         Delete ALL backup data from InfluxDB")
            print("  --json                Output current metrics as JSON (with background scan)")
            print("  --json-wait           Output metrics as JSON after scan completes")
            print("  --start-background-scan  Start background directory scan")
            print("  --verbose, -v         Enable verbose output")
            print("")
            print("Examples:")
            print("  --status                      # Quick status check (last run)")
            print("  --status --runs=5 --scope=all # Detailed status for last 5 runs")
            print("  --status --scope=all --runs=3     # Complete status report for last 3 runs")
            print("")
            print("Deduplication:")
            print("  The script tracks which backup sessions have been sent to InfluxDB")
            print("  State is stored in metrics-state.pkl and cleaned automatically")
            print("  Use --reset-state to force resending all available data")
            sys.exit(0)
    else:
        # Default: start background scan and output current JSON
        collector.start_background_directory_scan()
        metrics = collector.generate_metrics_fast()
        print(json.dumps(metrics, indent=2, default=lambda x: x.isoformat() if isinstance(x, datetime) else str(x)))


if __name__ == "__main__":
    main()